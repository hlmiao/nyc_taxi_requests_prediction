{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO download data and unzip archive file commands [Question 1]\n",
    "!aws s3 cp s3://nyc-tlc/trip\\ data/yellow_tripdata_2018-01.csv nyc-tlc/trip\\ data/yellow_tripdata_2018-01.csv\n",
    "!aws s3 cp s3://nyc-tlc/trip\\ data/yellow_tripdata_2018-02.csv nyc-tlc/trip\\ data/yellow_tripdata_2018-02.csv\n",
    "!aws s3 cp s3://nyc-tlc/trip\\ data/yellow_tripdata_2018-03.csv nyc-tlc/trip\\ data/yellow_tripdata_2018-03.csv\n",
    "!aws s3 cp s3://nyc-tlc/trip\\ data/yellow_tripdata_2018-04.csv nyc-tlc/trip\\ data/yellow_tripdata_2018-04.csv\n",
    "!aws s3 cp s3://nyc-tlc/trip\\ data/yellow_tripdata_2018-05.csv nyc-tlc/trip\\ data/yellow_tripdata_2018-05.csv\n",
    "!aws s3 cp s3://nyc-tlc/trip\\ data/yellow_tripdata_2018-06.csv nyc-tlc/trip\\ data/yellow_tripdata_2018-06.csv\n",
    "!aws s3 cp s3://nyc-tlc/misc/taxi\\ _zone_lookup.csv nyc-tlc/misc/taxi\\ _zone_lookup.csv\n",
    "!aws s3 cp s3://nyc-tlc/misc/taxi_zones.zip nyc-tlc/misc/taxi_zones.zip\n",
    "!cd nyc-tlc/misc/ && unzip taxi_zones.zip -d taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate [Question 1]\n",
    "!tree nyc-tlc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:\n",
    "\n",
    "```\n",
    "nyc-tlc\n",
    "├── misc\n",
    "│   ├── taxi\\ _zone_lookup.csv\n",
    "│   ├── taxi_zones\n",
    "│   │   ├── taxi_zones.dbf\n",
    "│   │   ├── taxi_zones.prj\n",
    "│   │   ├── taxi_zones.sbn\n",
    "│   │   ├── taxi_zones.sbx\n",
    "│   │   ├── taxi_zones.shp\n",
    "│   │   ├── taxi_zones.shp.xml\n",
    "│   │   └── taxi_zones.shx\n",
    "│   └── taxi_zones.zip\n",
    "└── trip\\ data\n",
    "    ├── yellow_tripdata_2018-01.csv\n",
    "    ├── yellow_tripdata_2018-02.csv\n",
    "    ├── yellow_tripdata_2018-03.csv\n",
    "    ├── yellow_tripdata_2018-04.csv\n",
    "    ├── yellow_tripdata_2018-05.csv\n",
    "    └── yellow_tripdata_2018-06.csv\n",
    "\n",
    "3 directories, 15 files\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Prepare\n",
    "\n",
    "We import all useful packages, and set the `first_datetime` to 2018-01-01 00:00:00, and `last_datetime` to 2018-07-01 00:00:00. We split the dataset into two parts: train and validate, by setting the `train_valid_split_datetime` to 2018-06-01 00:00:00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error as mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global setting\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_5min_id(x):\n",
    "    return int((x-first_datetime).total_seconds()//(5*60))\n",
    "\n",
    "def get_15min_id(x):\n",
    "    return int((x-first_datetime).total_seconds()//(15*60))\n",
    "\n",
    "def get_30min_id(x):\n",
    "    return int((x-first_datetime).total_seconds()//(30*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time setting\n",
    "first_datetime = datetime.datetime.strptime('2018-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "last_datetime = datetime.datetime.strptime('2018-07-01 00:00:00', '%Y-%m-%d %H:%M:%S')  # '2018-07-01 00:00:00'\n",
    "train_valid_split_datetime = datetime.datetime.strptime('2018-06-01 00:00:00', '%Y-%m-%d %H:%M:%S')  # '2018-06-01 00:00:00'\n",
    "print('first_datetime:', first_datetime)\n",
    "print('last_datetime:', last_datetime)\n",
    "print('train_valid_split_datetime:', train_valid_split_datetime)\n",
    "\n",
    "first_5min_id = get_5min_id(first_datetime)\n",
    "first_15min_id = get_15min_id(first_datetime)\n",
    "first_30min_id = get_30min_id(first_datetime)\n",
    "print('first_5min_id:', first_5min_id)\n",
    "print('first_15min_id:', first_15min_id)\n",
    "print('first_30min_id:', first_30min_id)\n",
    "\n",
    "last_5min_id = get_5min_id(last_datetime)\n",
    "last_15min_id = get_15min_id(last_datetime)\n",
    "last_30min_id = get_30min_id(last_datetime)\n",
    "print('last_5min_id:', last_5min_id)\n",
    "print('last_15min_id:', last_15min_id)\n",
    "print('last_30min_id:', last_30min_id)\n",
    "\n",
    "train_valid_split_5min_id = get_5min_id(train_valid_split_datetime)\n",
    "train_valid_split_15min_id = get_15min_id(train_valid_split_datetime)\n",
    "train_valid_split_30min_id = get_30min_id(train_valid_split_datetime)\n",
    "print('train_valid_split_5min_id:', train_valid_split_5min_id)\n",
    "print('train_valid_split_15min_id:', train_valid_split_15min_id)\n",
    "print('train_valid_split_30min_id:', train_valid_split_30min_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Zones\n",
    "\n",
    "Since newest NYC Taxi dataset only provides `PULocationID` and `DOLocationID`, instead of `pickup_longitude`, `pickup_latitude`, `dropoff_longitude`, and `dropoff_latitude`, we can only predict requests in each `PULocationID` (zone). We load [taxi _zone_lookup.csv] and [taxi_zones.shp], and use `geopandas` to visualize the zones in Manhattan (69 in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Manhattan taxi zone lookup\n",
    "taxi_zone_lookup = pd.read_csv('nyc-tlc/misc/taxi _zone_lookup.csv')\n",
    "print('taxi_zone_lookup:', taxi_zone_lookup.shape)\n",
    "manhattan_location_ids = taxi_zone_lookup[taxi_zone_lookup['Borough']=='Manhattan']['LocationID'].values\n",
    "manhattan_location_num = len(manhattan_location_ids)\n",
    "print('manhattan_location_ids:', manhattan_location_ids.shape, manhattan_location_ids)\n",
    "print('manhattan_location_num:', manhattan_location_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manhattan taxi zones shape\n",
    "taxi_zones_shape = gp.GeoDataFrame.from_file('nyc-tlc/misc/taxi_zones/taxi_zones.shp')\n",
    "taxi_zones_shape = taxi_zones_shape[taxi_zones_shape['borough'] == 'Manhattan']\n",
    "taxi_zones_shape.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize Manhattan taxi zones shape\n",
    "taxi_zones_shape.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepare\n",
    "\n",
    "We load all data from [nyc-tlc/trip data/] between Jan and June 2018, and filter abnormal data. We use `matplotlib` and `geopandas` to visualize some columns and help us to understand the trip data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter abnormal data: tpep_pickup_datetime, tpep_dropoff_datetime, trip_distance, trip duration, trip_speed, total_amount, etc.\n",
    "def filter_abnormal_data(sample):\n",
    "    start = time.time()\n",
    "    sample_manhattan = sample[sample['PULocationID'].isin(manhattan_location_ids)].copy()\n",
    "    print('filter PULocationID:', sample_manhattan.shape, time.time()-start)\n",
    "    sample_manhattan['tpep_pickup_datetime'] = pd.to_datetime(sample_manhattan['tpep_pickup_datetime'])\n",
    "    print('tpep_pickup_datetime:', time.time()-start)\n",
    "    sample_manhattan['tpep_dropoff_datetime'] = pd.to_datetime(sample_manhattan['tpep_dropoff_datetime'])\n",
    "    print('tpep_dropoff_datetime:', time.time()-start)\n",
    "    sample_manhattan = sample_manhattan[sample_manhattan['tpep_pickup_datetime'] >= first_datetime]\n",
    "    print('filter tpep_pickup_datetime first_datetime:', sample_manhattan.shape, time.time()-start)\n",
    "    sample_manhattan = sample_manhattan[sample_manhattan['tpep_pickup_datetime'] < last_datetime]\n",
    "    print('filter tpep_pickup_datetime last_datetime:', sample_manhattan.shape, time.time()-start)\n",
    "    sample_manhattan = sample_manhattan[sample_manhattan['trip_distance'] > 0]\n",
    "    print('filter trip_distance:', sample_manhattan.shape, time.time()-start)\n",
    "    sample_manhattan['trip_duration'] = (sample_manhattan['tpep_dropoff_datetime']-sample_manhattan['tpep_pickup_datetime']).dt.total_seconds()\n",
    "    print('trip_duration:', time.time()-start)\n",
    "    sample_manhattan = sample_manhattan[sample_manhattan['trip_duration'] > 0]\n",
    "    print('filter trip_duration:', sample_manhattan.shape, time.time()-start)\n",
    "    sample_manhattan['trip_speed'] = sample_manhattan['trip_distance']/sample_manhattan['trip_duration']*3600\n",
    "    print('trip_speed:', time.time()-start)\n",
    "    sample_manhattan = sample_manhattan[sample_manhattan['trip_speed'] > 0]\n",
    "    sample_manhattan = sample_manhattan[sample_manhattan['trip_speed'] <= 200]\n",
    "    print('filter trip_speed:', sample_manhattan.shape, time.time()-start)\n",
    "    sample_manhattan = sample_manhattan[sample_manhattan['total_amount'] > 0]\n",
    "    print('filter total_amount:', sample_manhattan.shape, time.time()-start)\n",
    "    return sample_manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Manhattan data: from 2018-01 to 2018-06, call filter_abnormal_data to filter data [Question 2]\n",
    "for m in range(1, 7):  # (1, 7)\n",
    "    start = time.time()\n",
    "    \n",
    "    # TODO call pandas function to read csv from csv file, return variable should be sample_1\n",
    "    sample_1 = pd.read_csv('nyc-tlc/trip data/yellow_tripdata_2018-0' + str(m) + '.csv')\n",
    "    \n",
    "    print('read_csv 2018-0'+str(m)+':', time.time()-start)\n",
    "    sample_1 = filter_abnormal_data(sample_1)\n",
    "    print('filter_abnormal_data:', time.time()-start)\n",
    "    if m == 1:\n",
    "        sample_manhattan = sample_1\n",
    "    else:\n",
    "        \n",
    "        # TODO concat sample_manhattan and sample_1\n",
    "        sample_manhattan = pd.concat([sample_manhattan, sample_1], axis=0)\n",
    "        \n",
    "    print('concat:', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate [Question 2]\n",
    "print('sample_manhattan:', sample_manhattan.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `sample_manhattan: (48730027, 19)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter too large total_amount, trip_distance, trip_duration\n",
    "start = time.time()\n",
    "\n",
    "m = np.mean(sample_manhattan['total_amount'])\n",
    "s = np.std(sample_manhattan['total_amount'])\n",
    "print('total_amount m:', m, 's:', s)\n",
    "sample_manhattan = sample_manhattan[sample_manhattan['total_amount'] <= m + 10*s]\n",
    "#sample_manhattan = sample_manhattan[sample_manhattan['total_amount'] >= m - 10*s]\n",
    "print('filter total_amount:', sample_manhattan.shape, time.time()-start)\n",
    "\n",
    "m = np.mean(sample_manhattan['trip_distance'])\n",
    "s = np.std(sample_manhattan['trip_distance'])\n",
    "print('trip_distance m:', m, 's:', s)\n",
    "sample_manhattan = sample_manhattan[sample_manhattan['trip_distance'] <= m + 20*s]\n",
    "#sample_manhattan = sample_manhattan[sample_manhattan['trip_distance'] >= m - 20*s]\n",
    "print('filter trip_distance:', sample_manhattan.shape, time.time()-start)\n",
    "\n",
    "m = np.mean(sample_manhattan['trip_duration'])\n",
    "s = np.std(sample_manhattan['trip_duration'])\n",
    "print('trip_duration m:', m, 's:', s)\n",
    "sample_manhattan = sample_manhattan[sample_manhattan['trip_duration'] <= m + 2*s]\n",
    "#sample_manhattan = sample_manhattan[sample_manhattan['trip_duration'] >= m - 2*s]\n",
    "print('filter trip_duration:', sample_manhattan.shape, time.time()-start)\n",
    "\n",
    "print('sample_manhattan.shape:', sample_manhattan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append, change and drop columns\n",
    "start = time.time()\n",
    "sample_manhattan['store_and_fwd_flag'] = sample_manhattan['store_and_fwd_flag'].map(lambda x: x == 'N' and 0 or 1)\n",
    "print('store_and_fwd_flag:', time.time()-start)\n",
    "\n",
    "sample_manhattan['tpep_pickup_5min_id'] = (sample_manhattan['tpep_pickup_datetime']-first_datetime).dt.total_seconds()//(5*60)\n",
    "print('tpep_pickup_5min_id:', time.time()-start)\n",
    "sample_manhattan['tpep_pickup_15min_id'] = (sample_manhattan['tpep_pickup_datetime']-first_datetime).dt.total_seconds()//(15*60)\n",
    "print('tpep_pickup_15min_id:', time.time()-start)\n",
    "sample_manhattan['tpep_pickup_30min_id'] = (sample_manhattan['tpep_pickup_datetime']-first_datetime).dt.total_seconds()//(30*60)\n",
    "print('tpep_pickup_30min_id:', time.time()-start)\n",
    "\n",
    "sample_manhattan.drop(['tpep_pickup_datetime', 'tpep_dropoff_datetime'], axis=1, inplace=True)  # , 'tpep_pickup_date', 'tpep_dropoff_date'\n",
    "print('sample_manhattan.shape:', sample_manhattan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show first 5 rows of sample_manhattan [Question 3.1]\n",
    "sample_manhattan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show statistics of sample_manhattan [Question 3.2]\n",
    "sample_manhattan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sample_manhattan['total_amount'].values, bins=100)\n",
    "plt.xlabel('total_amount')\n",
    "plt.ylabel('number of records')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_manhattan['log_total_amount'] = np.log(sample_manhattan['total_amount'].values + 1)\n",
    "plt.hist(sample_manhattan['log_total_amount'].values, bins=100)\n",
    "plt.xlabel('log(total_amount)')\n",
    "plt.ylabel('number of records')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sample_manhattan['trip_distance'].values, bins=100)\n",
    "plt.xlabel('trip_distance')\n",
    "plt.ylabel('number of records')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_manhattan['log_trip_distance'] = np.log(sample_manhattan['trip_distance'].values + 1)\n",
    "plt.hist(sample_manhattan['log_trip_distance'].values, bins=100)\n",
    "plt.xlabel('log(trip_distance)')\n",
    "plt.ylabel('number of records')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sample_manhattan['trip_duration'].values, bins=100)\n",
    "plt.xlabel('trip_duration')\n",
    "plt.ylabel('number of records')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_manhattan['log_trip_duration'] = np.log(sample_manhattan['trip_duration'].values + 1)\n",
    "plt.hist(sample_manhattan['log_trip_duration'].values, bins=100)\n",
    "plt.xlabel('log(trip_duration)')\n",
    "plt.ylabel('number of records')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sample_manhattan['trip_speed'].values, bins=100)\n",
    "plt.xlabel('trip_speed')\n",
    "plt.ylabel('number of records')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_manhattan['log_trip_speed'] = np.log(sample_manhattan['trip_speed'].values + 1)\n",
    "plt.hist(sample_manhattan['log_trip_speed'].values, bins=100)\n",
    "plt.xlabel('log(trip_speed)')\n",
    "plt.ylabel('number of records')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PULocationID_group = sample_manhattan.groupby(['PULocationID']).count()[['VendorID']]\n",
    "PULocationID_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zones_shape_requests = taxi_zones_shape.join(PULocationID_group, on=['LocationID'], how='left')\n",
    "taxi_zones_shape_requests.fillna(0, inplace=True)\n",
    "print('taxi_zones_shape_requests:', taxi_zones_shape_requests.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zones_shape_requests.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zones_shape_requests.plot(column='VendorID', cmap='OrRd', edgecolor='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Prepare\n",
    "\n",
    "We set the `5min_id`, `15min_id` and `30min_id` to represent 5min, 15min and 30min slot. For example, time between 2018-01-01 00:00:00 and 2018-01-01 00:05:00 has a `5min_id` as 0, and time between 2018-01-01 00:05:00 and 2018-01-01 00:10:00 has a `5min_id` as 1, and the similar with `15min_id` and `30min_id`. For each `Xmin_id` (X represents 5, 15 or 30), we predict the requests in all 69 zones. We have some `static features` such as `month`, `day`, `hour`, `weekday`, `is_weekend`, `is_morning_peak`, `is_evening_pick` for all `Xmin_id` and zones. Also we can extend more static features such as weather and zone features. Other `dynamic features` includes requests in `5min ago`, `10min ago`, `15min ago`, `7days ago`, etc. Also we can extend more dynamic features such as total passengers in 5min ago. At last, we generate 34 features for each `Xmin_id` and zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_all_index_and_static(last_id, id_name):\n",
    "    start = time.time()\n",
    "    all_id = np.array([i for i in range(int(last_id)) for _ in range(manhattan_location_num)])\n",
    "    all_LocationID = np.array([i for _ in range(int(last_id)) for i in manhattan_location_ids])\n",
    "    print('all_id:', all_id.shape, all_id)\n",
    "    print('all_LocationID:', all_LocationID.shape, all_LocationID)\n",
    "\n",
    "    all_index = pd.DataFrame({id_name: all_id, 'LocationID': all_LocationID})\n",
    "    all_index.set_index([id_name, 'LocationID'], inplace=True)\n",
    "    print('all_index:', all_index.shape)\n",
    "\n",
    "    all_static = pd.DataFrame({id_name: all_id, 'LocationID': all_LocationID})\n",
    "    all_static['tpep_pickup_datetime'] = pd.to_timedelta(all_static[id_name]*5*60, unit='s') + first_datetime\n",
    "    print('tpep_pickup_datetime:', time.time()-start)\n",
    "    #all_static['tpep_pickup_year'] = all_static['tpep_pickup_datetime'].dt.year\n",
    "    #print('tpep_pickup_year:', time.time()-start)\n",
    "    all_static['tpep_pickup_month'] = all_static['tpep_pickup_datetime'].dt.month\n",
    "    print('tpep_pickup_month:', time.time()-start)\n",
    "    all_static['tpep_pickup_day'] = all_static['tpep_pickup_datetime'].dt.day\n",
    "    print('tpep_pickup_day:', time.time()-start)\n",
    "    all_static['tpep_pickup_hour'] = all_static['tpep_pickup_datetime'].dt.hour\n",
    "    print('tpep_pickup_hour:', time.time()-start)\n",
    "    all_static['tpep_pickup_weekday'] = all_static['tpep_pickup_datetime'].dt.weekday\n",
    "    print('tpep_pickup_weekday:', time.time()-start)\n",
    "    all_static['is_weekend'] = all_static['tpep_pickup_weekday'].map(lambda x: x >= 5 and 1 or 0)\n",
    "    print('is_weekend:', time.time()-start)\n",
    "    all_static['is_morning_peak'] = all_static['tpep_pickup_hour'].map(lambda x: 7 <= x <= 9 and 1 or 0)\n",
    "    print('is_morning_peak:', time.time()-start)\n",
    "    all_static['is_evening_peak'] = all_static['tpep_pickup_hour'].map(lambda x: 17 <= x <= 19 and 1 or 0)\n",
    "    print('is_evening_peak:', time.time()-start)\n",
    "    all_static.drop(['tpep_pickup_datetime'], axis=1, inplace=True)\n",
    "    all_static.set_index([id_name, 'LocationID'], inplace=True)\n",
    "    print('all_static:', all_static.shape)\n",
    "    return all_index, all_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_index, all_5min_static = get_all_index_and_static(last_5min_id, 'tpep_pickup_5min_id')\n",
    "all_15min_index, all_15min_static = get_all_index_and_static(last_15min_id, 'tpep_pickup_15min_id')\n",
    "all_30min_index, all_30min_static = get_all_index_and_static(last_30min_id, 'tpep_pickup_30min_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_index.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_static.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_static.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_group(id_name):\n",
    "    sample_group = sample_manhattan.groupby([id_name, 'PULocationID'])\n",
    "    sample_count = sample_group.count()\n",
    "    sample_count.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_count:', sample_count.shape)\n",
    "    sample_mean = sample_group.mean()\n",
    "    sample_mean.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_mean:', sample_mean.shape)\n",
    "    sample_sum = sample_group.sum()\n",
    "    sample_sum.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_sum:', sample_sum.shape)\n",
    "    sample_dropoff_group = sample_manhattan.groupby([id_name, 'DOLocationID'])\n",
    "    sample_dropoff_count = sample_dropoff_group.count()\n",
    "    sample_dropoff_count.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_dropoff_count:', sample_dropoff_count.shape)\n",
    "    sample_dropoff_mean = sample_dropoff_group.mean()\n",
    "    sample_dropoff_mean.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_dropoff_mean:', sample_dropoff_mean.shape)\n",
    "    sample_dropoff_sum = sample_dropoff_group.sum()\n",
    "    sample_dropoff_sum.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_dropoff_sum:', sample_dropoff_sum.shape)\n",
    "    return sample_count, sample_mean, sample_sum, sample_dropoff_count, sample_dropoff_mean, sample_dropoff_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_count, sample_5min_mean, sample_5min_sum, sample_5min_dropoff_count, sample_5min_dropoff_mean, sample_5min_dropoff_sum = get_sample_group('tpep_pickup_5min_id')\n",
    "sample_15min_count, sample_15min_mean, sample_15min_sum, sample_15min_dropoff_count, sample_15min_dropoff_mean, sample_15min_dropoff_sum = get_sample_group('tpep_pickup_15min_id')\n",
    "sample_30min_count, sample_30min_mean, sample_30min_sum, sample_30min_dropoff_count, sample_30min_dropoff_mean, sample_30min_dropoff_sum = get_sample_group('tpep_pickup_30min_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_dropoff_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_dropoff_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_5min_dropoff_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(all_index, sample_count, sample_mean, sample_sum, sample_dropoff_count, sample_dropoff_mean, sample_dropoff_sum):\n",
    "    all_count = all_index.join(sample_count, how='left')\n",
    "    all_count.fillna(0, inplace=True)\n",
    "    print('all_count:', all_count.shape)\n",
    "    all_mean = all_index.join(sample_mean, how='left')\n",
    "    all_mean.fillna(0, inplace=True)\n",
    "    print('all_mean:', all_mean.shape)\n",
    "    all_sum = all_index.join(sample_sum, how='left')\n",
    "    all_sum.fillna(0, inplace=True)\n",
    "    print('all_sum:', all_sum.shape)\n",
    "    all_dropoff_count = all_index.join(sample_dropoff_count, how='left')\n",
    "    all_dropoff_count.fillna(0, inplace=True)\n",
    "    print('all_dropoff_count:', all_dropoff_count.shape)\n",
    "    all_dropoff_mean = all_index.join(sample_dropoff_mean, how='left')\n",
    "    all_dropoff_mean.fillna(0, inplace=True)\n",
    "    print('all_dropoff_mean:', all_dropoff_mean.shape)\n",
    "    all_dropoff_sum = all_index.join(sample_dropoff_sum, how='left')\n",
    "    all_dropoff_sum.fillna(0, inplace=True)\n",
    "    print('all_dropoff_sum:', all_dropoff_sum.shape)\n",
    "    all_xmin = all_count.copy()\n",
    "    all_xmin = all_xmin.join(all_mean, lsuffix='_count', rsuffix='_mean')\n",
    "    all_xmin = all_xmin.join(all_sum, rsuffix='_sum')\n",
    "    all_xmin = all_xmin.join(all_dropoff_count, rsuffix='_dropoff_count')\n",
    "    all_xmin = all_xmin.join(all_dropoff_mean, rsuffix='_dropoff_mean')\n",
    "    all_xmin = all_xmin.join(all_dropoff_sum, rsuffix='_dropoff_sum')\n",
    "    print('all_xmin:', all_xmin.shape)\n",
    "    return all_xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min = get_all(all_5min_index, sample_5min_count, sample_5min_mean, sample_5min_sum, sample_5min_dropoff_count, sample_5min_dropoff_mean, sample_5min_dropoff_sum)\n",
    "all_15min = get_all(all_15min_index, sample_15min_count, sample_15min_mean, sample_15min_sum, sample_15min_dropoff_count, sample_15min_dropoff_mean, sample_15min_dropoff_sum)\n",
    "all_30min = get_all(all_30min_index, sample_30min_count, sample_30min_mean, sample_30min_sum, sample_30min_dropoff_count, sample_30min_dropoff_mean, sample_30min_dropoff_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(all_xmin, all_static):\n",
    "    all_xmin_features = all_static.copy()\n",
    "    all_xmin_features['value'] = all_xmin['VendorID_count']\n",
    "    all_xmin_features['5min_ago'] = all_xmin['VendorID_count'].shift(manhattan_location_num)\n",
    "    all_xmin_features['5min_10min_ago'] = all_xmin['VendorID_count'].shift(2*manhattan_location_num)\n",
    "    all_xmin_features['10min_ago'] = all_xmin_features['5min_ago'] + all_xmin_features['5min_10min_ago']\n",
    "    all_xmin_features['10min_15min_ago'] = all_xmin['VendorID_count'].shift(3*manhattan_location_num)\n",
    "    all_xmin_features['15min_ago'] = all_xmin_features['10min_ago'] + all_xmin_features['10min_15min_ago']\n",
    "    all_xmin_features['15min_20min_ago'] = all_xmin['VendorID_count'].shift(4*manhattan_location_num)\n",
    "    all_xmin_features['20min_ago'] = all_xmin_features['15min_ago'] + all_xmin_features['15min_20min_ago']\n",
    "    all_xmin_features['20min_25min_ago'] = all_xmin['VendorID_count'].shift(5*manhattan_location_num)\n",
    "    all_xmin_features['25min_ago'] = all_xmin_features['20min_ago'] + all_xmin_features['20min_25min_ago']\n",
    "    all_xmin_features['25min_30min_ago'] = all_xmin['VendorID_count'].shift(6*manhattan_location_num)\n",
    "    all_xmin_features['30min_ago'] = all_xmin_features['25min_ago'] + all_xmin_features['25min_30min_ago']\n",
    "    all_xmin_features['5min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(manhattan_location_num)\n",
    "    all_xmin_features['5min_10min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(2*manhattan_location_num)\n",
    "    all_xmin_features['10min_ago_drop'] = all_xmin_features['5min_ago_drop'] + all_xmin_features['5min_10min_ago_drop']\n",
    "    all_xmin_features['10min_15min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(3*manhattan_location_num)\n",
    "    all_xmin_features['15min_ago_drop'] = all_xmin_features['10min_ago_drop'] + all_xmin_features['10min_15min_ago_drop']\n",
    "    all_xmin_features['15min_20min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(4*manhattan_location_num)\n",
    "    all_xmin_features['20min_ago_drop'] = all_xmin_features['15min_ago_drop'] + all_xmin_features['15min_20min_ago_drop']\n",
    "    all_xmin_features['20min_25min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(5*manhattan_location_num)\n",
    "    all_xmin_features['25min_ago_drop'] = all_xmin_features['20min_ago_drop'] + all_xmin_features['20min_25min_ago_drop']\n",
    "    all_xmin_features['25min_30min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(6*manhattan_location_num)\n",
    "    all_xmin_features['30min_ago_drop'] = all_xmin_features['25min_ago_drop'] + all_xmin_features['25min_30min_ago_drop']\n",
    "    all_xmin_features['1day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24)\n",
    "    all_xmin_features['7day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24*7)\n",
    "    all_xmin_features['14day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24*14)\n",
    "    all_xmin_features['21day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24*21)\n",
    "    all_xmin_features['28day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24*28)\n",
    "    all_xmin_features.fillna(0, inplace=True)\n",
    "    print('all_xmin_features:', all_xmin_features.shape)\n",
    "    return all_xmin_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_features = get_all_features(all_5min, all_5min_static)\n",
    "all_15min_features = get_all_features(all_15min, all_15min_static)\n",
    "all_30min_features = get_all_features(all_30min, all_30min_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate\n",
    "\n",
    "We split all data into train and validate part. We demonstrate 4 methods to forecast requests: XGBoost, LightGBM, linear regression implemented using sklearn and linear regression implemented using TensorFlow, and evaluate the models using mean absolute error (MAE). We also visualize the prediction results between 2018-01-01 00:00:00 and 2018-01-01 00:05:00 using `geopandas` (the darker the color, the more demand), and we can visualize any time slot using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_5min = all_5min_features[:int(train_valid_split_5min_id)*manhattan_location_num]\n",
    "print('train_X_5min:', train_X_5min.shape)\n",
    "valid_X_5min = all_5min_features[int(train_valid_split_5min_id)*manhattan_location_num:int(last_5min_id)*manhattan_location_num]\n",
    "print('valid_X_5min:', valid_X_5min.shape)\n",
    "train_Y_5min = train_X_5min['value'].values\n",
    "print('train_Y_5min:', len(train_Y_5min))\n",
    "valid_Y_5min = valid_X_5min['value'].values\n",
    "print('valid_Y_5min:', len(valid_Y_5min))\n",
    "\n",
    "train_X_15min = all_15min_features[:int(train_valid_split_15min_id)*manhattan_location_num]\n",
    "print('train_X_15min:', train_X_15min.shape)\n",
    "valid_X_15min = all_15min_features[int(train_valid_split_15min_id)*manhattan_location_num:int(last_15min_id)*manhattan_location_num]\n",
    "print('valid_X_15min:', valid_X_15min.shape)\n",
    "train_Y_15min = train_X_15min['value'].values\n",
    "print('train_Y_15min:', len(train_Y_15min))\n",
    "valid_Y_15min = valid_X_15min['value'].values\n",
    "print('valid_Y_15min:', len(valid_Y_15min))\n",
    "\n",
    "train_X_30min = all_30min_features[:int(train_valid_split_30min_id)*manhattan_location_num]\n",
    "print('train_X_30min:', train_X_30min.shape)\n",
    "valid_X_30min = all_30min_features[int(train_valid_split_30min_id)*manhattan_location_num:int(last_30min_id)*manhattan_location_num]\n",
    "print('valid_X_30min:', valid_X_30min.shape)\n",
    "train_Y_30min = train_X_30min['value'].values\n",
    "print('train_Y_30min:', len(train_Y_30min))\n",
    "valid_Y_30min = valid_X_30min['value'].values\n",
    "print('valid_Y_30min:', len(valid_Y_30min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((train_X_5min, valid_X_5min, train_Y_5min, valid_Y_5min), open('train_valid_5min.pickle', 'wb'), protocol=2)\n",
    "pickle.dump((train_X_15min, valid_X_15min, train_Y_15min, valid_Y_15min), open('train_valid_15min.pickle', 'wb'), protocol=2)\n",
    "pickle.dump((train_X_30min, valid_X_30min, train_Y_30min, valid_Y_30min), open('train_valid_30min.pickle', 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_5min, valid_X_5min, train_Y_5min, valid_Y_5min = pickle.load(open('train_valid_5min.pickle', 'rb'))\n",
    "train_X_15min, valid_X_15min, train_Y_15min, valid_Y_15min = pickle.load(open('train_valid_15min.pickle', 'rb'))\n",
    "train_X_30min, valid_X_30min, train_Y_30min, valid_Y_30min = pickle.load(open('train_valid_30min.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_5min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_5min.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X_5min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X_5min.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    xg_train = xgb.DMatrix(train_X.drop('value', axis=1), label=train_Y)\n",
    "    xg_test = xgb.DMatrix(test_X.drop('value', axis=1), label=test_Y)\n",
    "    # setup parameters for xgboost\n",
    "    param = {}\n",
    "    # scale weight of positive examples\n",
    "    param['eta'] = 0.1  # default\n",
    "    param['max_depth'] = 6  # default: 6\n",
    "    param['silent'] = 1  # default\n",
    "    param['nthread'] = 4  # default\n",
    "    param['gamma'] = 1\n",
    "    param['subsample'] = 0.9\n",
    "    param['min_child_weight'] = 1\n",
    "    param['colsample_bytree'] = 0.9\n",
    "    param['lambda'] = 1\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['eval_metric'] = 'mae'\n",
    "    param['objective'] = 'reg:linear'\n",
    "    \n",
    "    watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "    num_round = 100\n",
    "\n",
    "    bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "\n",
    "    imp = bst.get_fscore()\n",
    "    print(sorted(imp.items(), key=lambda d: d[1], reverse=True))\n",
    "    \n",
    "    pred = bst.predict(xg_test)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    rfc = LinearRegression()\n",
    "    rfc.fit(train_X.drop('value', axis=1), train_Y.astype(np.float))\n",
    "    pred = rfc.predict(test_X.drop('value', axis=1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=tf.nn.relu, input_shape=[train_X_5min.shape[1]-1]),\n",
    "        layers.Dense(64, activation=tf.nn.relu),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.001)\n",
    "\n",
    "    model.compile(loss='mae', optimizer=optimizer, metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "def tf_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    EPOCHS = 10\n",
    "    model = build_model()\n",
    "    model.summary()\n",
    "    history = model.fit(train_X.drop('value', axis=1), train_Y, epochs=EPOCHS, validation_split = 0.2, verbose=2)\n",
    "    loss, mae, mse = model.evaluate(test_X.drop('value', axis=1), test_Y, verbose=1)\n",
    "    pred = model.predict(test_X.drop('value', axis=1)).flatten()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    # create dataset for lightgbm\n",
    "    lgb_train = lgb.Dataset(train_X.drop('value', axis=1), train_Y)\n",
    "    lgb_eval = lgb.Dataset(test_X.drop('value', axis=1), test_Y, reference=lgb_train)\n",
    "\n",
    "    # specify your configurations as a dict\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': {'l2', 'l1'},\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    print('Starting training...')\n",
    "    # train\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=100,\n",
    "                    valid_sets=lgb_eval,\n",
    "                    early_stopping_rounds=5)\n",
    "    \n",
    "    print('Starting predicting...')\n",
    "    # predict\n",
    "    pred = gbm.predict(test_X.drop('value', axis=1), num_iteration=gbm.best_iteration)\n",
    "    # eval\n",
    "    print('The mae of prediction is:', mae(test_Y, pred))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new prediction algorithm or change parameters of above 4 prediction algorithms [Challenge Question]\n",
    "def new_algo_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    \"\"\"\n",
    "    :param train_X : Dataframe, (?, 35) train data including 'value' column, you should drop the column first (already done)\n",
    "    :param train_Y: array, train label data, which is actually train_X['value'].values\n",
    "    :param test_X : Dataframe, (?, 35) test data including 'value' column, you should drop the column first (already done)\n",
    "    :param test_Y: array, test label data, which is actually test_X['value'].values\n",
    "    :return: array, test prediction data\n",
    "    \"\"\"\n",
    "    train_X = train_X.drop('value', axis=1)\n",
    "    test_X = test_X.drop('value', axis=1)\n",
    "    pred = np.array([0 for _ in test_Y])\n",
    "    \n",
    "    # TODO implement your prediction algorithm here\n",
    "    \n",
    "    \n",
    "    return pred\n",
    "\n",
    "pred_30min_new_algo = new_algo_train_validate(train_X_30min, train_Y_30min, valid_X_30min, valid_Y_30min)\n",
    "valid_30min_new_algo_mae = mae(valid_Y_30min, pred_30min_new_algo)\n",
    "print('valid_30min_new_algo_mae:', valid_30min_new_algo_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate 5min slot\n",
    "pred_5min_xgb = xgb_train_validate(train_X_5min, train_Y_5min, valid_X_5min, valid_Y_5min)\n",
    "valid_5min_xgb_mae = mae(valid_Y_5min, pred_5min_xgb)\n",
    "print('valid_5min_xgb_mae:', valid_5min_xgb_mae)\n",
    "pred_5min_lr = lr_train_validate(train_X_5min, train_Y_5min, valid_X_5min, valid_Y_5min)\n",
    "valid_5min_lr_mae = mae(valid_Y_5min, pred_5min_lr)\n",
    "print('valid_5min_lr_mae:', valid_5min_lr_mae)\n",
    "#pred_5min_tf = tf_train_validate(train_X_5min, train_Y_5min, valid_X_5min, valid_Y_5min)\n",
    "#valid_5min_tf_mae = mae(valid_Y_5min, pred_5min_tf)\n",
    "#print('valid_5min_tf_mae:', valid_5min_tf_mae)\n",
    "pred_5min_lgb = lgb_train_validate(train_X_5min, train_Y_5min, valid_X_5min, valid_Y_5min)\n",
    "valid_5min_lgb_mae = mae(valid_Y_5min, pred_5min_lgb)\n",
    "print('valid_5min_lgb_mae:', valid_5min_lgb_mae)\n",
    "valid_pred_5min = pd.DataFrame(valid_X_5min, columns=['value'])\n",
    "valid_pred_5min.reset_index(inplace=True)\n",
    "valid_pred_5min['pred_xgb'] = pred_5min_xgb\n",
    "valid_pred_5min['pred_lr'] = pred_5min_lr\n",
    "#valid_pred_5min['pred_tf'] = pred_5min_tf\n",
    "valid_pred_5min['pred_lgb'] = pred_5min_lgb\n",
    "print('valid_pred_5min:', valid_pred_5min.shape)\n",
    "train_X_5min.to_csv('train_X_5min.csv', index=True)\n",
    "valid_X_5min.to_csv('valid_X_5min.csv', index=True)\n",
    "valid_pred_5min.to_csv('valid_pred_5min.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate 15min slot\n",
    "pred_15min_xgb = xgb_train_validate(train_X_15min, train_Y_15min, valid_X_15min, valid_Y_15min)\n",
    "valid_15min_xgb_mae = mae(valid_Y_15min, pred_15min_xgb)\n",
    "print('valid_15min_xgb_mae:', valid_15min_xgb_mae)\n",
    "pred_15min_lr = lr_train_validate(train_X_15min, train_Y_15min, valid_X_15min, valid_Y_15min)\n",
    "valid_15min_lr_mae = mae(valid_Y_15min, pred_15min_lr)\n",
    "print('valid_15min_lr_mae:', valid_15min_lr_mae)\n",
    "#pred_15min_tf = tf_train_validate(train_X_15min, train_Y_15min, valid_X_15min, valid_Y_15min)\n",
    "#valid_15min_tf_mae = mae(valid_Y_15min, pred_15min_tf)\n",
    "#print('valid_15min_tf_mae:', valid_15min_tf_mae)\n",
    "pred_15min_lgb = lgb_train_validate(train_X_15min, train_Y_15min, valid_X_15min, valid_Y_15min)\n",
    "valid_15min_lgb_mae = mae(valid_Y_15min, pred_15min_lgb)\n",
    "print('valid_15min_lgb_mae:', valid_15min_lgb_mae)\n",
    "valid_pred_15min = pd.DataFrame(valid_X_15min, columns=['value'])\n",
    "valid_pred_15min.reset_index(inplace=True)\n",
    "valid_pred_15min['pred_xgb'] = pred_15min_xgb\n",
    "valid_pred_15min['pred_lr'] = pred_15min_lr\n",
    "#valid_pred_15min['pred_tf'] = pred_15min_tf\n",
    "valid_pred_15min['pred_lgb'] = pred_15min_lgb\n",
    "print('valid_pred_15min:', valid_pred_15min.shape)\n",
    "train_X_15min.to_csv('train_X_15min.csv', index=True)\n",
    "valid_X_15min.to_csv('valid_X_15min.csv', index=True)\n",
    "valid_pred_15min.to_csv('valid_pred_15min.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate 30min slot\n",
    "pred_30min_xgb = xgb_train_validate(train_X_30min, train_Y_30min, valid_X_30min, valid_Y_30min)\n",
    "valid_30min_xgb_mae = mae(valid_Y_30min, pred_30min_xgb)\n",
    "print('valid_30min_xgb_mae:', valid_30min_xgb_mae)\n",
    "pred_30min_lr = lr_train_validate(train_X_30min, train_Y_30min, valid_X_30min, valid_Y_30min)\n",
    "valid_30min_lr_mae = mae(valid_Y_30min, pred_30min_lr)\n",
    "print('valid_30min_lr_mae:', valid_30min_lr_mae)\n",
    "#pred_30min_tf = tf_train_validate(train_X_30min, train_Y_30min, valid_X_30min, valid_Y_30min)\n",
    "#valid_30min_tf_mae = mae(valid_Y_30min, pred_30min_tf)\n",
    "#print('valid_30min_tf_mae:', valid_30min_tf_mae)\n",
    "pred_30min_lgb = lgb_train_validate(train_X_30min, train_Y_30min, valid_X_30min, valid_Y_30min)\n",
    "valid_30min_lgb_mae = mae(valid_Y_30min, pred_30min_lgb)\n",
    "print('valid_30min_lgb_mae:', valid_30min_lgb_mae)\n",
    "valid_pred_30min = pd.DataFrame(valid_X_30min, columns=['value'])\n",
    "valid_pred_30min.reset_index(inplace=True)\n",
    "valid_pred_30min['pred_xgb'] = pred_30min_xgb\n",
    "valid_pred_30min['pred_lr'] = pred_30min_lr\n",
    "#valid_pred_30min['pred_tf'] = pred_30min_tf\n",
    "valid_pred_30min['pred_lgb'] = pred_30min_lgb\n",
    "print('valid_pred_30min:', valid_pred_30min.shape)\n",
    "train_X_30min.to_csv('train_X_30min.csv', index=True)\n",
    "valid_X_30min.to_csv('valid_X_30min.csv', index=True)\n",
    "valid_pred_30min.to_csv('valid_pred_30min.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_5min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_name, group_data in valid_pred_5min.groupby(['tpep_pickup_5min_id']):\n",
    "    print('group_name:', group_name)\n",
    "    valid_pred_5min_group = group_data.groupby(['LocationID']).sum()[['value', 'pred_xgb', 'pred_lr', 'pred_lgb']]  # , 'pred_tf'\n",
    "    taxi_zones_shape_requests_pred_5min = taxi_zones_shape.join(valid_pred_5min_group, on=['LocationID'], how='left')\n",
    "    taxi_zones_shape_requests_pred_5min.fillna(0, inplace=True)\n",
    "    taxi_zones_shape_requests_pred_5min.plot(column='value', cmap='OrRd', edgecolor='white')\n",
    "    taxi_zones_shape_requests_pred_5min.plot(column='pred_xgb', cmap='OrRd', edgecolor='white')\n",
    "    taxi_zones_shape_requests_pred_5min.plot(column='pred_lr', cmap='OrRd', edgecolor='white')\n",
    "    #taxi_zones_shape_requests_pred_5min.plot(column='pred_tf', cmap='OrRd', edgecolor='white')\n",
    "    taxi_zones_shape_requests_pred_5min.plot(column='pred_lgb', cmap='OrRd', edgecolor='white')\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_5min_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show evaluate result\n",
    "print('valid_5min_xgb_mae:', valid_5min_xgb_mae)\n",
    "print('valid_5min_lr_mae:', valid_5min_lr_mae)\n",
    "#print('valid_5min_tf_mae:', valid_5min_tf_mae)\n",
    "print('valid_5min_lgb_mae:', valid_5min_lgb_mae)\n",
    "\n",
    "print('valid_15min_xgb_mae:', valid_15min_xgb_mae)\n",
    "print('valid_15min_lr_mae:', valid_15min_lr_mae)\n",
    "#print('valid_15min_tf_mae:', valid_15min_tf_mae)\n",
    "print('valid_15min_lgb_mae:', valid_15min_lgb_mae)\n",
    "\n",
    "print('valid_30min_xgb_mae:', valid_30min_xgb_mae)\n",
    "print('valid_30min_lr_mae:', valid_30min_lr_mae)\n",
    "#print('valid_30min_tf_mae:', valid_30min_tf_mae)\n",
    "print('valid_30min_lgb_mae:', valid_30min_lgb_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The validate results show that XGBoost performs better than other algorithms. We can improve the results by adding more features and carefully tuning the parameters of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
