{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Question 1]\n",
    "TODO download data and unzip archive file commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here:\n",
    "!aws s3 cp s3://nyc-tlc/trip\\ data/yellow_tripdata_2018-04.csv nyc_tlc/trip_data/yellow_tripdata_2018-04.csv\n",
    "!aws s3 cp s3://nyc-tlc/trip\\ data/yellow_tripdata_2018-05.csv nyc_tlc/trip_data/yellow_tripdata_2018-05.csv\n",
    "!aws s3 cp s3://nyc-tlc/trip\\ data/yellow_tripdata_2018-06.csv nyc_tlc/trip_data/yellow_tripdata_2018-06.csv\n",
    "!aws s3 cp s3://nyc-tlc/misc/taxi\\ _zone_lookup.csv nyc_tlc/misc/taxi_zone_lookup.csv\n",
    "!aws s3 cp s3://nyc-tlc/misc/taxi_zones.zip nyc_tlc/misc/taxi_zones.zip\n",
    "!cd nyc_tlc/misc/ && rm -rf taxi_zones && unzip taxi_zones.zip -d taxi_zones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree nyc_tlc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Preparation\n",
    "\n",
    "We import all useful packages, do some basic/global settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "import logging\n",
    "\n",
    "import contest_helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global setting\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "plt.rcParams['agg.path.chunksize'] = 10000\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Zones Shape Preparation\n",
    "\n",
    "Since newest NYC Taxi dataset only provides `PULocationID` and `DOLocationID`, instead of `pickup_longitude`, `pickup_latitude`, `dropoff_longitude`, and `dropoff_latitude`, we can only predict requests in each `PULocationID` (zone). We load [taxi_zone_lookup.csv] and [taxi_zones.shp], and use `geopandas` to visualize the zones in Manhattan (69 in total).\n",
    "\n",
    "contest_helper.NycTaxiAnalyzer is a wrapper class to load and present taxi data and zones shape.\n",
    "1. contest_helper.NycTaxiAnalyzer.taxi_zone_lookup: pandas.DataFrame\n",
    "1. contest_helper.NycTaxiAnalyzer.taxi_zones_shape: geopandas.GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxi_analyzer = contest_helper.NycTaxiAnalyzer()\n",
    "\n",
    "nyc_taxi_analyzer.load_shape('nyc_tlc/misc/taxi_zone_lookup.csv',\n",
    "                            'nyc_tlc/misc/taxi_zones/taxi_zones.shp',\n",
    "                            borough='Manhattan')\n",
    "\n",
    "nyc_taxi_analyzer.taxi_zones_shape.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Question 2]\n",
    "TODO:\n",
    "1. load Manhattan data: from 2018-04 to 2018-06\n",
    "1. define a function 'filter_abnormal_data' to filter abnormal data in 'contest_helper.NycTaxiAnalyzer.data'\n",
    "1. call filter_abnormal_data to filter 'contest_helper.NycTaxiAnalyzer.data'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We split the dataset into two parts: train and validate by setting `train_valid_split_datetime` to 2018-06-01 00:00:00.\n",
    "We set `first_datetime` to 2018-04-01 00:00:00, and `last_datetime` to 2018-07-01 00:00:00.\n",
    "We load all data from [nyc_tlc/trip_data/] between `first_datetime` and `last_datetime`.\n",
    "We use `matplotlib` and `geopandas` to visualize some columns and help us to understand the trip data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_datetime '2018-04-01 00:00:00'\n",
    "fd = datetime.datetime.strptime('2018-04-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "# last_datetime '2018-07-01 00:00:00'\n",
    "ld = datetime.datetime.strptime('2018-07-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "# train_valid_split_datetime '2018-06-01 00:00:00'\n",
    "tvsd = datetime.datetime.strptime('2018-06-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "nyc_taxi_analyzer.load_data('nyc_tlc/trip_data/', first_datetime=fd, last_datetime=ld)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter abnormal data\n",
    "\n",
    "Define a function, and filter abnormal data.\n",
    "Acceptable data should be validated like below,\n",
    "1. trip_distance > 0\n",
    "1. trip_duration > 0\n",
    "1. trip_speed <= 200\n",
    "1. total_amount > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxi_analyzer.data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution goes here:\n",
    "\n",
    "def filter_abnormal_data(data):\n",
    "    return data\n",
    "\n",
    "\n",
    "nyc_taxi_analyzer.filter_abnormal_data()\n",
    "sample = filter_abnormal_data(nyc_taxi_analyzer.data)\n",
    "\n",
    "sample.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate [Question 2], you should get `sample.shape: (24540246, 23)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Question 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [Question 3.1] Show statistics of the prepared sample data \n",
    "nyc_taxi_analyzer.data.describe()\n",
    "\n",
    "# your solution goes here:\n",
    "\n",
    "sample = nyc_taxi_analyzer.data\n",
    "#sample['log_total_amount'] = np.log(sample['total_amount'].values + 1)\n",
    "plt.hist(sample['trip_speed'].values, bins=100)\n",
    "plt.xlabel('speed')\n",
    "plt.ylabel('number of records')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [Question 3.2] Caculate and present hit-map of taxi needs per PULocationID\n",
    "nyc_taxi_analyzer.data.head()\n",
    "\n",
    "\n",
    "# your solution goes here:\n",
    "\n",
    "PULocationID_group = nyc_taxi_analyzer.data.groupby(['PULocationID']).count()[['VendorID']]\n",
    "PULocationID_group\n",
    "\n",
    "taxi_zones_shape_requests = nyc_taxi_analyzer.taxi_zones_shape.join(PULocationID_group, on=['LocationID'], how='left')\n",
    "taxi_zones_shape_requests.fillna(0, inplace=True)\n",
    "print('taxi_zones_shape_requests:', taxi_zones_shape_requests.shape)\n",
    "\n",
    "taxi_zones_shape_requests.head()\n",
    "taxi_zones_shape_requests.plot(column='VendorID', cmap='OrRd', edgecolor='white')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Challenge Question]\n",
    "\n",
    "TODO: Add new prediction algorithm or change parameters of below 4 prediction algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Prepare\n",
    "\n",
    "We set the `5min_id`, `15min_id` and `30min_id` to represent 5min, 15min and 30min slot. For example, time between 2018-01-01 00:00:00 and 2018-01-01 00:05:00 has a `5min_id` as 0, and time between 2018-01-01 00:05:00 and 2018-01-01 00:10:00 has a `5min_id` as 1, and the similar with `15min_id` and `30min_id`. For each `Xmin_id` (X represents 5, 15 or 30), we predict the requests in all 69 zones. We have some `static features` such as `month`, `day`, `hour`, `weekday`, `is_weekend`, `is_morning_peak`, `is_evening_pick` for all `Xmin_id` and zones. Also we can extend more static features such as weather and zone features. Other `dynamic features` includes requests in `5min ago`, `10min ago`, `15min ago`, `7days ago`, etc. Also we can extend more dynamic features such as total passengers in 5min ago. At last, we generate 34 features for each `Xmin_id` and zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_5min_id = nyc_taxi_analyzer.get_5min_id(fd)\n",
    "first_15min_id = nyc_taxi_analyzer.get_15min_id(fd)\n",
    "first_30min_id = nyc_taxi_analyzer.get_30min_id(fd)\n",
    "\n",
    "\n",
    "last_5min_id = nyc_taxi_analyzer.get_5min_id(ld)\n",
    "last_15min_id = nyc_taxi_analyzer.get_15min_id(ld)\n",
    "last_30min_id = nyc_taxi_analyzer.get_30min_id(ld)\n",
    "\n",
    "\n",
    "train_valid_split_5min_id = nyc_taxi_analyzer.get_5min_id(tvsd)\n",
    "train_valid_split_15min_id = nyc_taxi_analyzer.get_15min_id(tvsd)\n",
    "train_valid_split_30min_id = nyc_taxi_analyzer.get_30min_id(tvsd)\n",
    "\n",
    "all_5min_index, all_5min_static = nyc_taxi_analyzer.get_all_index_and_static(last_5min_id, 'tpep_pickup_5min_id')\n",
    "all_15min_index, all_15min_static = nyc_taxi_analyzer.get_all_index_and_static(last_15min_id, 'tpep_pickup_15min_id')\n",
    "all_30min_index, all_30min_static = nyc_taxi_analyzer.get_all_index_and_static(last_30min_id, 'tpep_pickup_30min_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_index.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_static.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_static.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_group(id_name, sample_manhattan):\n",
    "    sample_group = sample_manhattan.groupby([id_name, 'PULocationID'])\n",
    "    sample_count = sample_group.count()\n",
    "    sample_count.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_count:', sample_count.shape)\n",
    "    sample_mean = sample_group.mean()\n",
    "    sample_mean.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_mean:', sample_mean.shape)\n",
    "    sample_sum = sample_group.sum()\n",
    "    sample_sum.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_sum:', sample_sum.shape)\n",
    "    sample_dropoff_group = sample_manhattan.groupby([id_name, 'DOLocationID'])\n",
    "    sample_dropoff_count = sample_dropoff_group.count()\n",
    "    sample_dropoff_count.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_dropoff_count:', sample_dropoff_count.shape)\n",
    "    sample_dropoff_mean = sample_dropoff_group.mean()\n",
    "    sample_dropoff_mean.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_dropoff_mean:', sample_dropoff_mean.shape)\n",
    "    sample_dropoff_sum = sample_dropoff_group.sum()\n",
    "    sample_dropoff_sum.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_dropoff_sum:', sample_dropoff_sum.shape)\n",
    "    return sample_count, sample_mean, sample_sum, sample_dropoff_count, sample_dropoff_mean, sample_dropoff_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_count, sample_5min_mean, sample_5min_sum, sample_5min_dropoff_count, sample_5min_dropoff_mean, sample_5min_dropoff_sum = get_sample_group('tpep_pickup_5min_id', nyc_taxi_analyzer.data)\n",
    "sample_15min_count, sample_15min_mean, sample_15min_sum, sample_15min_dropoff_count, sample_15min_dropoff_mean, sample_15min_dropoff_sum = get_sample_group('tpep_pickup_15min_id', nyc_taxi_analyzer.data)\n",
    "sample_30min_count, sample_30min_mean, sample_30min_sum, sample_30min_dropoff_count, sample_30min_dropoff_mean, sample_30min_dropoff_sum = get_sample_group('tpep_pickup_30min_id', nyc_taxi_analyzer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_dropoff_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_dropoff_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_5min_dropoff_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(all_index, sample_count, sample_mean, sample_sum, sample_dropoff_count, sample_dropoff_mean, sample_dropoff_sum):\n",
    "    all_count = all_index.join(sample_count, how='left')\n",
    "    all_count.fillna(0, inplace=True)\n",
    "    print('all_count:', all_count.shape)\n",
    "    all_mean = all_index.join(sample_mean, how='left')\n",
    "    all_mean.fillna(0, inplace=True)\n",
    "    print('all_mean:', all_mean.shape)\n",
    "    all_sum = all_index.join(sample_sum, how='left')\n",
    "    all_sum.fillna(0, inplace=True)\n",
    "    print('all_sum:', all_sum.shape)\n",
    "    all_dropoff_count = all_index.join(sample_dropoff_count, how='left')\n",
    "    all_dropoff_count.fillna(0, inplace=True)\n",
    "    print('all_dropoff_count:', all_dropoff_count.shape)\n",
    "    all_dropoff_mean = all_index.join(sample_dropoff_mean, how='left')\n",
    "    all_dropoff_mean.fillna(0, inplace=True)\n",
    "    print('all_dropoff_mean:', all_dropoff_mean.shape)\n",
    "    all_dropoff_sum = all_index.join(sample_dropoff_sum, how='left')\n",
    "    all_dropoff_sum.fillna(0, inplace=True)\n",
    "    print('all_dropoff_sum:', all_dropoff_sum.shape)\n",
    "    all_xmin = all_count.copy()\n",
    "    all_xmin = all_xmin.join(all_mean, lsuffix='_count', rsuffix='_mean')\n",
    "    all_xmin = all_xmin.join(all_sum, rsuffix='_sum')\n",
    "    all_xmin = all_xmin.join(all_dropoff_count, rsuffix='_dropoff_count')\n",
    "    all_xmin = all_xmin.join(all_dropoff_mean, rsuffix='_dropoff_mean')\n",
    "    all_xmin = all_xmin.join(all_dropoff_sum, rsuffix='_dropoff_sum')\n",
    "    print('all_xmin:', all_xmin.shape)\n",
    "    return all_xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min = get_all(all_5min_index, sample_5min_count, sample_5min_mean, sample_5min_sum, sample_5min_dropoff_count, sample_5min_dropoff_mean, sample_5min_dropoff_sum)\n",
    "all_15min = get_all(all_15min_index, sample_15min_count, sample_15min_mean, sample_15min_sum, sample_15min_dropoff_count, sample_15min_dropoff_mean, sample_15min_dropoff_sum)\n",
    "all_30min = get_all(all_30min_index, sample_30min_count, sample_30min_mean, sample_30min_sum, sample_30min_dropoff_count, sample_30min_dropoff_mean, sample_30min_dropoff_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(all_xmin, all_static, manhattan_location_num):\n",
    "    all_xmin_features = all_static.copy()\n",
    "    all_xmin_features['value'] = all_xmin['VendorID_count']\n",
    "    all_xmin_features['5min_ago'] = all_xmin['VendorID_count'].shift(manhattan_location_num)\n",
    "    all_xmin_features['5min_10min_ago'] = all_xmin['VendorID_count'].shift(2*manhattan_location_num)\n",
    "    all_xmin_features['10min_ago'] = all_xmin_features['5min_ago'] + all_xmin_features['5min_10min_ago']\n",
    "    all_xmin_features['10min_15min_ago'] = all_xmin['VendorID_count'].shift(3*manhattan_location_num)\n",
    "    all_xmin_features['15min_ago'] = all_xmin_features['10min_ago'] + all_xmin_features['10min_15min_ago']\n",
    "    all_xmin_features['15min_20min_ago'] = all_xmin['VendorID_count'].shift(4*manhattan_location_num)\n",
    "    all_xmin_features['20min_ago'] = all_xmin_features['15min_ago'] + all_xmin_features['15min_20min_ago']\n",
    "    all_xmin_features['20min_25min_ago'] = all_xmin['VendorID_count'].shift(5*manhattan_location_num)\n",
    "    all_xmin_features['25min_ago'] = all_xmin_features['20min_ago'] + all_xmin_features['20min_25min_ago']\n",
    "    all_xmin_features['25min_30min_ago'] = all_xmin['VendorID_count'].shift(6*manhattan_location_num)\n",
    "    all_xmin_features['30min_ago'] = all_xmin_features['25min_ago'] + all_xmin_features['25min_30min_ago']\n",
    "    all_xmin_features['5min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(manhattan_location_num)\n",
    "    all_xmin_features['5min_10min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(2*manhattan_location_num)\n",
    "    all_xmin_features['10min_ago_drop'] = all_xmin_features['5min_ago_drop'] + all_xmin_features['5min_10min_ago_drop']\n",
    "    all_xmin_features['10min_15min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(3*manhattan_location_num)\n",
    "    all_xmin_features['15min_ago_drop'] = all_xmin_features['10min_ago_drop'] + all_xmin_features['10min_15min_ago_drop']\n",
    "    all_xmin_features['15min_20min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(4*manhattan_location_num)\n",
    "    all_xmin_features['20min_ago_drop'] = all_xmin_features['15min_ago_drop'] + all_xmin_features['15min_20min_ago_drop']\n",
    "    all_xmin_features['20min_25min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(5*manhattan_location_num)\n",
    "    all_xmin_features['25min_ago_drop'] = all_xmin_features['20min_ago_drop'] + all_xmin_features['20min_25min_ago_drop']\n",
    "    all_xmin_features['25min_30min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(6*manhattan_location_num)\n",
    "    all_xmin_features['30min_ago_drop'] = all_xmin_features['25min_ago_drop'] + all_xmin_features['25min_30min_ago_drop']\n",
    "    all_xmin_features['1day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24)\n",
    "    all_xmin_features['7day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24*7)\n",
    "    all_xmin_features['14day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24*14)\n",
    "    all_xmin_features['21day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24*21)\n",
    "    all_xmin_features['28day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24*28)\n",
    "    all_xmin_features.fillna(0, inplace=True)\n",
    "    print('all_xmin_features:', all_xmin_features.shape)\n",
    "    return all_xmin_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_features = get_all_features(all_5min, all_5min_static, nyc_taxi_analyzer.location_num)\n",
    "all_15min_features = get_all_features(all_15min, all_15min_static, nyc_taxi_analyzer.location_num)\n",
    "all_30min_features = get_all_features(all_30min, all_30min_static, nyc_taxi_analyzer.location_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate\n",
    "\n",
    "We split all data into train and validate part. We demonstrate 4 methods to forecast requests: XGBoost, LightGBM, linear regression implemented using sklearn and linear regression implemented using TensorFlow, and evaluate the models using mean absolute error (MAE). We also visualize the prediction results between 2018-01-01 00:00:00 and 2018-01-01 00:05:00 using `geopandas` (the darker the color, the more demand), and we can visualize any time slot using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_location_num = nyc_taxi_analyzer.location_num\n",
    "\n",
    "train_X_5min = all_5min_features[:int(train_valid_split_5min_id)*manhattan_location_num]\n",
    "print('train_X_5min:', train_X_5min.shape)\n",
    "valid_X_5min = all_5min_features[int(train_valid_split_5min_id)*manhattan_location_num:int(last_5min_id)*manhattan_location_num]\n",
    "print('valid_X_5min:', valid_X_5min.shape)\n",
    "train_Y_5min = train_X_5min['value'].values\n",
    "print('train_Y_5min:', len(train_Y_5min))\n",
    "valid_Y_5min = valid_X_5min['value'].values\n",
    "print('valid_Y_5min:', len(valid_Y_5min))\n",
    "\n",
    "train_X_15min = all_15min_features[:int(train_valid_split_15min_id)*manhattan_location_num]\n",
    "print('train_X_15min:', train_X_15min.shape)\n",
    "valid_X_15min = all_15min_features[int(train_valid_split_15min_id)*manhattan_location_num:int(last_15min_id)*manhattan_location_num]\n",
    "print('valid_X_15min:', valid_X_15min.shape)\n",
    "train_Y_15min = train_X_15min['value'].values\n",
    "print('train_Y_15min:', len(train_Y_15min))\n",
    "valid_Y_15min = valid_X_15min['value'].values\n",
    "print('valid_Y_15min:', len(valid_Y_15min))\n",
    "\n",
    "train_X_30min = all_30min_features[:int(train_valid_split_30min_id)*manhattan_location_num]\n",
    "print('train_X_30min:', train_X_30min.shape)\n",
    "valid_X_30min = all_30min_features[int(train_valid_split_30min_id)*manhattan_location_num:int(last_30min_id)*manhattan_location_num]\n",
    "print('valid_X_30min:', valid_X_30min.shape)\n",
    "train_Y_30min = train_X_30min['value'].values\n",
    "print('train_Y_30min:', len(train_Y_30min))\n",
    "valid_Y_30min = valid_X_30min['value'].values\n",
    "print('valid_Y_30min:', len(valid_Y_30min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((train_X_5min, valid_X_5min, train_Y_5min, valid_Y_5min), open('train_valid_5min.pickle', 'wb'), protocol=2)\n",
    "pickle.dump((train_X_15min, valid_X_15min, train_Y_15min, valid_Y_15min), open('train_valid_15min.pickle', 'wb'), protocol=2)\n",
    "pickle.dump((train_X_30min, valid_X_30min, train_Y_30min, valid_Y_30min), open('train_valid_30min.pickle', 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_5min, valid_X_5min, train_Y_5min, valid_Y_5min = pickle.load(open('train_valid_5min.pickle', 'rb'))\n",
    "train_X_15min, valid_X_15min, train_Y_15min, valid_Y_15min = pickle.load(open('train_valid_15min.pickle', 'rb'))\n",
    "train_X_30min, valid_X_30min, train_Y_30min, valid_Y_30min = pickle.load(open('train_valid_30min.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_5min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_5min.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X_5min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X_5min.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    xg_train = xgb.DMatrix(train_X.drop('value', axis=1), label=train_Y)\n",
    "    xg_test = xgb.DMatrix(test_X.drop('value', axis=1), label=test_Y)\n",
    "    # setup parameters for xgboost\n",
    "    param = {}\n",
    "    # scale weight of positive examples\n",
    "    param['eta'] = 0.1  # default\n",
    "    param['max_depth'] = 6  # default: 6\n",
    "    param['silent'] = 1  # default\n",
    "    param['nthread'] = 4  # default\n",
    "    param['gamma'] = 1\n",
    "    param['subsample'] = 0.9\n",
    "    param['min_child_weight'] = 1\n",
    "    param['colsample_bytree'] = 0.9\n",
    "    param['lambda'] = 1\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['eval_metric'] = 'mae'\n",
    "    param['objective'] = 'reg:linear'\n",
    "    \n",
    "    watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "    num_round = 100\n",
    "\n",
    "    bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "\n",
    "    imp = bst.get_fscore()\n",
    "    print(sorted(imp.items(), key=lambda d: d[1], reverse=True))\n",
    "    \n",
    "    pred = bst.predict(xg_test)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    rfc = LinearRegression()\n",
    "    rfc.fit(train_X.drop('value', axis=1), train_Y.astype(np.float))\n",
    "    pred = rfc.predict(test_X.drop('value', axis=1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=tf.nn.relu, input_shape=[train_X_5min.shape[1]-1]),\n",
    "        layers.Dense(64, activation=tf.nn.relu),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.001)\n",
    "\n",
    "    model.compile(loss='mae', optimizer=optimizer, metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "def tf_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    EPOCHS = 10\n",
    "    model = build_model()\n",
    "    model.summary()\n",
    "    history = model.fit(train_X.drop('value', axis=1), train_Y, epochs=EPOCHS, validation_split = 0.2, verbose=2)\n",
    "    loss, mae, mse = model.evaluate(test_X.drop('value', axis=1), test_Y, verbose=1)\n",
    "    pred = model.predict(test_X.drop('value', axis=1)).flatten()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    # create dataset for lightgbm\n",
    "    lgb_train = lgb.Dataset(train_X.drop('value', axis=1), train_Y)\n",
    "    lgb_eval = lgb.Dataset(test_X.drop('value', axis=1), test_Y, reference=lgb_train)\n",
    "\n",
    "    # specify your configurations as a dict\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': {'l2', 'l1'},\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    print('Starting training...')\n",
    "    # train\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=100,\n",
    "                    valid_sets=lgb_eval,\n",
    "                    early_stopping_rounds=5)\n",
    "    \n",
    "    print('Starting predicting...')\n",
    "    # predict\n",
    "    pred = gbm.predict(test_X.drop('value', axis=1), num_iteration=gbm.best_iteration)\n",
    "    # eval\n",
    "    print('The mae of prediction is:', mae(test_Y, pred))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new prediction algorithm or change parameters of above 4 prediction algorithms [Challenge Question]\n",
    "def new_algo_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    \"\"\"\n",
    "    :param train_X : Dataframe, (?, 35) train data including 'value' column, you should drop the column first (already done)\n",
    "    :param train_Y: array, train label data, which is actually train_X['value'].values\n",
    "    :param test_X : Dataframe, (?, 35) test data including 'value' column, you should drop the column first (already done)\n",
    "    :param test_Y: array, test label data, which is actually test_X['value'].values\n",
    "    :return: array, test prediction data\n",
    "    \"\"\"\n",
    "    train_X = train_X.drop('value', axis=1)\n",
    "    test_X = test_X.drop('value', axis=1)\n",
    "    pred = np.array([0 for _ in test_Y])\n",
    "    \n",
    "    # TODO implement your prediction algorithm here\n",
    "    \n",
    "    \n",
    "    return pred\n",
    "\n",
    "pred_30min_new_algo = new_algo_train_validate(train_X_30min, train_Y_30min, valid_X_30min, valid_Y_30min)\n",
    "valid_30min_new_algo_mae = mae(valid_Y_30min, pred_30min_new_algo)\n",
    "print('valid_30min_new_algo_mae:', valid_30min_new_algo_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate 5min slot\n",
    "pred_5min_xgb = xgb_train_validate(train_X_5min, train_Y_5min, valid_X_5min, valid_Y_5min)\n",
    "valid_5min_xgb_mae = mae(valid_Y_5min, pred_5min_xgb)\n",
    "print('valid_5min_xgb_mae:', valid_5min_xgb_mae)\n",
    "pred_5min_lr = lr_train_validate(train_X_5min, train_Y_5min, valid_X_5min, valid_Y_5min)\n",
    "valid_5min_lr_mae = mae(valid_Y_5min, pred_5min_lr)\n",
    "print('valid_5min_lr_mae:', valid_5min_lr_mae)\n",
    "#pred_5min_tf = tf_train_validate(train_X_5min, train_Y_5min, valid_X_5min, valid_Y_5min)\n",
    "#valid_5min_tf_mae = mae(valid_Y_5min, pred_5min_tf)\n",
    "#print('valid_5min_tf_mae:', valid_5min_tf_mae)\n",
    "pred_5min_lgb = lgb_train_validate(train_X_5min, train_Y_5min, valid_X_5min, valid_Y_5min)\n",
    "valid_5min_lgb_mae = mae(valid_Y_5min, pred_5min_lgb)\n",
    "print('valid_5min_lgb_mae:', valid_5min_lgb_mae)\n",
    "valid_pred_5min = pd.DataFrame(valid_X_5min, columns=['value'])\n",
    "valid_pred_5min.reset_index(inplace=True)\n",
    "valid_pred_5min['pred_xgb'] = pred_5min_xgb\n",
    "valid_pred_5min['pred_lr'] = pred_5min_lr\n",
    "#valid_pred_5min['pred_tf'] = pred_5min_tf\n",
    "valid_pred_5min['pred_lgb'] = pred_5min_lgb\n",
    "print('valid_pred_5min:', valid_pred_5min.shape)\n",
    "train_X_5min.to_csv('train_X_5min.csv', index=True)\n",
    "valid_X_5min.to_csv('valid_X_5min.csv', index=True)\n",
    "valid_pred_5min.to_csv('valid_pred_5min.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate 15min slot\n",
    "pred_15min_xgb = xgb_train_validate(train_X_15min, train_Y_15min, valid_X_15min, valid_Y_15min)\n",
    "valid_15min_xgb_mae = mae(valid_Y_15min, pred_15min_xgb)\n",
    "print('valid_15min_xgb_mae:', valid_15min_xgb_mae)\n",
    "pred_15min_lr = lr_train_validate(train_X_15min, train_Y_15min, valid_X_15min, valid_Y_15min)\n",
    "valid_15min_lr_mae = mae(valid_Y_15min, pred_15min_lr)\n",
    "print('valid_15min_lr_mae:', valid_15min_lr_mae)\n",
    "#pred_15min_tf = tf_train_validate(train_X_15min, train_Y_15min, valid_X_15min, valid_Y_15min)\n",
    "#valid_15min_tf_mae = mae(valid_Y_15min, pred_15min_tf)\n",
    "#print('valid_15min_tf_mae:', valid_15min_tf_mae)\n",
    "pred_15min_lgb = lgb_train_validate(train_X_15min, train_Y_15min, valid_X_15min, valid_Y_15min)\n",
    "valid_15min_lgb_mae = mae(valid_Y_15min, pred_15min_lgb)\n",
    "print('valid_15min_lgb_mae:', valid_15min_lgb_mae)\n",
    "valid_pred_15min = pd.DataFrame(valid_X_15min, columns=['value'])\n",
    "valid_pred_15min.reset_index(inplace=True)\n",
    "valid_pred_15min['pred_xgb'] = pred_15min_xgb\n",
    "valid_pred_15min['pred_lr'] = pred_15min_lr\n",
    "#valid_pred_15min['pred_tf'] = pred_15min_tf\n",
    "valid_pred_15min['pred_lgb'] = pred_15min_lgb\n",
    "print('valid_pred_15min:', valid_pred_15min.shape)\n",
    "train_X_15min.to_csv('train_X_15min.csv', index=True)\n",
    "valid_X_15min.to_csv('valid_X_15min.csv', index=True)\n",
    "valid_pred_15min.to_csv('valid_pred_15min.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate 30min slot\n",
    "pred_30min_xgb = xgb_train_validate(train_X_30min, train_Y_30min, valid_X_30min, valid_Y_30min)\n",
    "valid_30min_xgb_mae = mae(valid_Y_30min, pred_30min_xgb)\n",
    "print('valid_30min_xgb_mae:', valid_30min_xgb_mae)\n",
    "pred_30min_lr = lr_train_validate(train_X_30min, train_Y_30min, valid_X_30min, valid_Y_30min)\n",
    "valid_30min_lr_mae = mae(valid_Y_30min, pred_30min_lr)\n",
    "print('valid_30min_lr_mae:', valid_30min_lr_mae)\n",
    "#pred_30min_tf = tf_train_validate(train_X_30min, train_Y_30min, valid_X_30min, valid_Y_30min)\n",
    "#valid_30min_tf_mae = mae(valid_Y_30min, pred_30min_tf)\n",
    "#print('valid_30min_tf_mae:', valid_30min_tf_mae)\n",
    "pred_30min_lgb = lgb_train_validate(train_X_30min, train_Y_30min, valid_X_30min, valid_Y_30min)\n",
    "valid_30min_lgb_mae = mae(valid_Y_30min, pred_30min_lgb)\n",
    "print('valid_30min_lgb_mae:', valid_30min_lgb_mae)\n",
    "valid_pred_30min = pd.DataFrame(valid_X_30min, columns=['value'])\n",
    "valid_pred_30min.reset_index(inplace=True)\n",
    "valid_pred_30min['pred_xgb'] = pred_30min_xgb\n",
    "valid_pred_30min['pred_lr'] = pred_30min_lr\n",
    "#valid_pred_30min['pred_tf'] = pred_30min_tf\n",
    "valid_pred_30min['pred_lgb'] = pred_30min_lgb\n",
    "print('valid_pred_30min:', valid_pred_30min.shape)\n",
    "train_X_30min.to_csv('train_X_30min.csv', index=True)\n",
    "valid_X_30min.to_csv('valid_X_30min.csv', index=True)\n",
    "valid_pred_30min.to_csv('valid_pred_30min.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_5min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group_name, group_data in valid_pred_5min.groupby(['tpep_pickup_5min_id']):\n",
    "    print('group_name:', group_name)\n",
    "    valid_pred_5min_group = group_data.groupby(['LocationID']).sum()[['value', 'pred_xgb', 'pred_lr', 'pred_lgb']]  # , 'pred_tf'\n",
    "    taxi_zones_shape_requests_pred_5min = nyc_taxi_analyzer.taxi_zones_shape.join(valid_pred_5min_group, on=['LocationID'], how='left')\n",
    "    taxi_zones_shape_requests_pred_5min.fillna(0, inplace=True)\n",
    "    taxi_zones_shape_requests_pred_5min.plot(column='value', cmap='OrRd', edgecolor='white')\n",
    "    taxi_zones_shape_requests_pred_5min.plot(column='pred_xgb', cmap='OrRd', edgecolor='white')\n",
    "    taxi_zones_shape_requests_pred_5min.plot(column='pred_lr', cmap='OrRd', edgecolor='white')\n",
    "    #taxi_zones_shape_requests_pred_5min.plot(column='pred_tf', cmap='OrRd', edgecolor='white')\n",
    "    taxi_zones_shape_requests_pred_5min.plot(column='pred_lgb', cmap='OrRd', edgecolor='white')\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_5min_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show evaluate result\n",
    "print('valid_5min_xgb_mae:', valid_5min_xgb_mae)\n",
    "print('valid_5min_lr_mae:', valid_5min_lr_mae)\n",
    "#print('valid_5min_tf_mae:', valid_5min_tf_mae)\n",
    "print('valid_5min_lgb_mae:', valid_5min_lgb_mae)\n",
    "\n",
    "print('valid_15min_xgb_mae:', valid_15min_xgb_mae)\n",
    "print('valid_15min_lr_mae:', valid_15min_lr_mae)\n",
    "#print('valid_15min_tf_mae:', valid_15min_tf_mae)\n",
    "print('valid_15min_lgb_mae:', valid_15min_lgb_mae)\n",
    "\n",
    "print('valid_30min_xgb_mae:', valid_30min_xgb_mae)\n",
    "print('valid_30min_lr_mae:', valid_30min_lr_mae)\n",
    "#print('valid_30min_tf_mae:', valid_30min_tf_mae)\n",
    "print('valid_30min_lgb_mae:', valid_30min_lgb_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The validate results show that XGBoost performs better than other algorithms. We can improve the results by adding more features and carefully tuning the parameters of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
