{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Question 1]\n",
    "\n",
    "Download data and unzip archive file commands.\n",
    "\n",
    "Validate [Question 1], you should get:\n",
    "\n",
    "```\n",
    "nyc_tlc\n",
    "├── misc\n",
    "│   ├── taxi_zone_lookup.csv\n",
    "│   ├── taxi_zones\n",
    "│   │   ├── taxi_zones.dbf\n",
    "│   │   ├── taxi_zones.prj\n",
    "│   │   ├── taxi_zones.sbn\n",
    "│   │   ├── taxi_zones.sbx\n",
    "│   │   ├── taxi_zones.shp\n",
    "│   │   ├── taxi_zones.shp.xml\n",
    "│   │   └── taxi_zones.shx\n",
    "│   └── taxi_zones.zip\n",
    "└── trip_data\n",
    "    ├── yellow_tripdata_2018-04.csv\n",
    "    ├── yellow_tripdata_2018-05.csv\n",
    "    └── yellow_tripdata_2018-06.csv\n",
    "\n",
    "3 directories, 12 files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO your solution goes here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree nyc_tlc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Preparation\n",
    "\n",
    "We import all useful packages, do some basic/global settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import logging\n",
    "import contest_helper\n",
    "\n",
    "# global setting\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "plt.rcParams['agg.path.chunksize'] = 10000\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Zones Shape Preparation\n",
    "\n",
    "Since newest NYC Taxi dataset only provides `PULocationID` and `DOLocationID`, instead of `pickup_longitude`, `pickup_latitude`, `dropoff_longitude`, and `dropoff_latitude`, we can only predict requests in each `PULocationID` (zone). We load [taxi_zone_lookup.csv] and [taxi_zones.shp], and use `geopandas` to visualize the zones in Manhattan (69 in total).\n",
    "\n",
    "contest_helper.NycTaxiAnalyzer is a wrapper class to load and present taxi data and zones shape.\n",
    "1. contest_helper.NycTaxiAnalyzer.taxi_zone_lookup: pandas.DataFrame\n",
    "1. contest_helper.NycTaxiAnalyzer.taxi_zones_shape: geopandas.GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_taxi_analyzer = contest_helper.NycTaxiAnalyzer()\n",
    "\n",
    "nyc_taxi_analyzer.load_shape('nyc_tlc/misc/taxi_zone_lookup.csv',\n",
    "                            'nyc_tlc/misc/taxi_zones/taxi_zones.shp',\n",
    "                            borough='Manhattan')\n",
    "\n",
    "nyc_taxi_analyzer.taxi_zones_shape.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Question 2]\n",
    "\n",
    "1. load Manhattan data: from 2018-04 to 2018-06\n",
    "2. define a function 'filter_abnormal_data' to filter abnormal data\n",
    "3. call filter_abnormal_data to filter 'contest_helper.NycTaxiAnalyzer.data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We split the dataset into two parts: train and validate by setting `train_valid_split_datetime` to 2018-06-01 00:00:00.\n",
    "We set `first_datetime` to 2018-04-01 00:00:00, and `last_datetime` to 2018-07-01 00:00:00.\n",
    "We load all data from [nyc_tlc/trip_data/] between `first_datetime` and `last_datetime`.\n",
    "We use `matplotlib` and `geopandas` to visualize some columns and help us to understand the trip data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_datetime '2018-04-01 00:00:00'\n",
    "fd = datetime.datetime.strptime('2018-04-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "# last_datetime '2018-07-01 00:00:00'\n",
    "ld = datetime.datetime.strptime('2018-07-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "# train_valid_split_datetime '2018-06-01 00:00:00'\n",
    "tvsd = datetime.datetime.strptime('2018-06-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "nyc_taxi_analyzer.load_data('nyc_tlc/trip_data/', first_datetime=fd, last_datetime=ld)\n",
    "nyc_taxi_analyzer.data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter abnormal data\n",
    "\n",
    "Define a function, and filter abnormal data.\n",
    "Acceptable data should be validated like below,\n",
    "1. trip_distance > 0\n",
    "1. trip_duration > 0\n",
    "1. 0 < trip_speed <= 200\n",
    "1. total_amount > 0\n",
    "\n",
    "Validate [Question 2], you should get: \n",
    "\n",
    "```\n",
    "(24540246, 23)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_abnormal_data(data):\n",
    "    # TODO your solution goes here:\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "nyc_taxi_analyzer.data = filter_abnormal_data(nyc_taxi_analyzer.data)\n",
    "\n",
    "nyc_taxi_analyzer.data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Question 3]\n",
    "\n",
    "Show statistics of the prepared sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO your solution goes here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# your solution goes here:\n",
    "\n",
    "def filter_abnormal_data(data):\n",
    "    return data\n",
    "\n",
    "\n",
    "sample = filter_abnormal_data(nyc_taxi_analyzer.data)\n",
    "\n",
    "sample.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Challenge Question]\n",
    "\n",
    "Add new prediction algorithm or change parameters of below 4 prediction algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_30min_id = nyc_taxi_analyzer.get_30min_id(fd)\n",
    "last_30min_id = nyc_taxi_analyzer.get_30min_id(ld)\n",
    "train_valid_split_30min_id = nyc_taxi_analyzer.get_30min_id(tvsd)\n",
    "all_30min_index, all_30min_static = nyc_taxi_analyzer.get_all_index_and_static(last_30min_id, 'tpep_pickup_30min_id')\n",
    "\n",
    "sample_30min_count, sample_30min_mean, sample_30min_sum, sample_30min_dropoff_count, sample_30min_dropoff_mean, sample_30min_dropoff_sum = nyc_taxi_analyzer.get_sample_group('tpep_pickup_30min_id', nyc_taxi_analyzer.data)\n",
    "all_30min = nyc_taxi_analyzer.get_all(all_30min_index, sample_30min_count, sample_30min_mean, sample_30min_sum, sample_30min_dropoff_count, sample_30min_dropoff_mean, sample_30min_dropoff_sum)\n",
    "all_30min_features = nyc_taxi_analyzer.get_all_features(all_30min, all_30min_static, nyc_taxi_analyzer.location_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate\n",
    "\n",
    "We split all data into train and validate part. We demonstrate 3 methods to forecast requests: XGBoost, LightGBM, linear regression implemented using sklearn, and evaluate the models using mean absolute error (MAE). We also visualize the prediction results between 2018-04-01 00:00:00 and 2018-04-01 00:05:00 using `geopandas` (the darker the color, the more demand), and we can visualize any time slot using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhattan_location_num = nyc_taxi_analyzer.location_num\n",
    "\n",
    "train_X_30min = all_30min_features[:int(train_valid_split_30min_id)*manhattan_location_num]\n",
    "print('train_X_30min:', train_X_30min.shape)\n",
    "valid_X_30min = all_30min_features[int(train_valid_split_30min_id)*manhattan_location_num:int(last_30min_id)*manhattan_location_num]\n",
    "print('valid_X_30min:', valid_X_30min.shape)\n",
    "train_Y_30min = train_X_30min['value'].values\n",
    "print('train_Y_30min:', len(train_Y_30min))\n",
    "valid_Y_30min = valid_X_30min['value'].values\n",
    "print('valid_Y_30min:', len(valid_Y_30min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    xg_train = xgb.DMatrix(train_X.drop('value', axis=1), label=train_Y)\n",
    "    xg_test = xgb.DMatrix(test_X.drop('value', axis=1), label=test_Y)\n",
    "    # setup parameters for xgboost\n",
    "    param = {}\n",
    "    # scale weight of positive examples\n",
    "    param['eta'] = 0.1  # default\n",
    "    param['max_depth'] = 6  # default: 6\n",
    "    param['silent'] = 1  # default\n",
    "    param['nthread'] = 4  # default\n",
    "    param['gamma'] = 1\n",
    "    param['subsample'] = 0.9\n",
    "    param['min_child_weight'] = 1\n",
    "    param['colsample_bytree'] = 0.9\n",
    "    param['lambda'] = 1\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['eval_metric'] = 'mae'\n",
    "    param['objective'] = 'reg:linear'\n",
    "    \n",
    "    watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "    num_round = 100\n",
    "\n",
    "    bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "\n",
    "    imp = bst.get_fscore()\n",
    "    print(sorted(imp.items(), key=lambda d: d[1], reverse=True))\n",
    "    \n",
    "    pred = bst.predict(xg_test)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    rfc = LinearRegression()\n",
    "    rfc.fit(train_X.drop('value', axis=1), train_Y.astype(np.float))\n",
    "    pred = rfc.predict(test_X.drop('value', axis=1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    # create dataset for lightgbm\n",
    "    lgb_train = lgb.Dataset(train_X.drop('value', axis=1), train_Y)\n",
    "    lgb_eval = lgb.Dataset(test_X.drop('value', axis=1), test_Y, reference=lgb_train)\n",
    "\n",
    "    # specify your configurations as a dict\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': {'l2', 'l1'},\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    print('Starting training...')\n",
    "    # train\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=100,\n",
    "                    valid_sets=lgb_eval,\n",
    "                    early_stopping_rounds=5)\n",
    "    \n",
    "    print('Starting predicting...')\n",
    "    # predict\n",
    "    pred = gbm.predict(test_X.drop('value', axis=1), num_iteration=gbm.best_iteration)\n",
    "    # eval\n",
    "    print('The mae of prediction is:', mae(test_Y, pred))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new prediction algorithm\n",
    "def new_algo_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    \"\"\"\n",
    "    :param train_X : Dataframe, (?, 35) train data including 'value' column, you should drop the column first (already done)\n",
    "    :param train_Y: array, train label data, which is actually train_X['value'].values\n",
    "    :param test_X : Dataframe, (?, 35) test data including 'value' column, you should drop the column first (already done)\n",
    "    :param test_Y: array, test label data, which is actually test_X['value'].values\n",
    "    :return: array, test prediction data\n",
    "    \"\"\"\n",
    "    train_X = train_X.drop('value', axis=1)\n",
    "    test_X = test_X.drop('value', axis=1)\n",
    "    pred = np.array([0 for _ in test_Y])\n",
    "    \n",
    "    # TODO your solution goes here:\n",
    "    \n",
    "    \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and validate 30min slot\n",
    "pred_30min_xgb = xgb_train_validate(train_X_30min, train_Y_30min, valid_X_30min, valid_Y_30min)\n",
    "valid_30min_xgb_mae = mae(valid_Y_30min, pred_30min_xgb)\n",
    "print('valid_30min_xgb_mae:', valid_30min_xgb_mae)\n",
    "pred_30min_lr = lr_train_validate(train_X_30min, train_Y_30min, valid_X_30min, valid_Y_30min)\n",
    "valid_30min_lr_mae = mae(valid_Y_30min, pred_30min_lr)\n",
    "print('valid_30min_lr_mae:', valid_30min_lr_mae)\n",
    "pred_30min_lgb = lgb_train_validate(train_X_30min, train_Y_30min, valid_X_30min, valid_Y_30min)\n",
    "valid_30min_lgb_mae = mae(valid_Y_30min, pred_30min_lgb)\n",
    "print('valid_30min_lgb_mae:', valid_30min_lgb_mae)\n",
    "pred_30min_new_algo = new_algo_train_validate(train_X_30min, train_Y_30min, valid_X_30min, valid_Y_30min)\n",
    "valid_30min_new_algo_mae = mae(valid_Y_30min, pred_30min_new_algo)\n",
    "print('valid_30min_new_algo_mae:', valid_30min_new_algo_mae)\n",
    "valid_pred_30min = pd.DataFrame(valid_X_30min, columns=['value'])\n",
    "valid_pred_30min.reset_index(inplace=True)\n",
    "valid_pred_30min['pred_xgb'] = pred_30min_xgb\n",
    "valid_pred_30min['pred_lr'] = pred_30min_lr\n",
    "valid_pred_30min['pred_lgb'] = pred_30min_lgb\n",
    "valid_pred_30min['pred_new_algo'] = pred_30min_lgb\n",
    "print('valid_pred_30min:', valid_pred_30min.shape)\n",
    "\n",
    "train_X_30min.to_csv('train_X_30min.csv', index=True)\n",
    "valid_X_30min.to_csv('valid_X_30min.csv', index=True)\n",
    "valid_pred_30min.to_csv('valid_pred_30min.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show evaluate result\n",
    "print('valid_30min_xgb_mae:', valid_30min_xgb_mae)\n",
    "print('valid_30min_lr_mae:', valid_30min_lr_mae)\n",
    "print('valid_30min_lgb_mae:', valid_30min_lgb_mae)\n",
    "print('valid_30min_new_algo_mae:', valid_30min_new_algo_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
