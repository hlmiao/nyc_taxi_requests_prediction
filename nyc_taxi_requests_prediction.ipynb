{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Question 1]\n",
    "TODO download data and unzip archive file commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please put your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate [Question 1], you should get:\n",
    "\n",
    "```\n",
    "nyc_tlc\n",
    "├── misc\n",
    "│   ├── taxi_zone_lookup.csv\n",
    "│   ├── taxi_zones\n",
    "│   │   ├── taxi_zones.dbf\n",
    "│   │   ├── taxi_zones.prj\n",
    "│   │   ├── taxi_zones.sbn\n",
    "│   │   ├── taxi_zones.sbx\n",
    "│   │   ├── taxi_zones.shp\n",
    "│   │   ├── taxi_zones.shp.xml\n",
    "│   │   └── taxi_zones.shx\n",
    "│   └── taxi_zones.zip\n",
    "└── trip_data\n",
    "    ├── yellow_tripdata_2018-04.csv\n",
    "    ├── yellow_tripdata_2018-05.csv\n",
    "    └── yellow_tripdata_2018-06.csv\n",
    "\n",
    "3 directories, 12 files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree nyc_tlc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "\n",
    "import contest_helper\n",
    "\n",
    "# global setting\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdt = datetime.datetime.strptime('2018-03-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "ldt = datetime.datetime.strptime('2018-07-01 00:00:00', '%Y-%m-%d %H:%M:%S')  # '2018-07-01 00:00:00'\n",
    "\n",
    "nyc_taxi_analyzer = contest_helper.NycTaxiAnalyzer('nyc_tlc/misc/taxi_zone_lookup.csv',\n",
    "                                                   'nyc_tlc/misc/taxi_zones/taxi_zones.shp',\n",
    "                                                  first_datetime=fdt, last_datetime=ldt)\n",
    "\n",
    "train_valid_split_datetime = datetime.datetime.strptime('2018-06-01 00:00:00', '%Y-%m-%d %H:%M:%S')  # '2018-06-01 00:00:00'\n",
    "\n",
    "\n",
    "first_5min_id = nyc_taxi_analyzer.get_5min_id(fdt)\n",
    "first_15min_id = nyc_taxi_analyzer.get_15min_id(fdt)\n",
    "first_30min_id = nyc_taxi_analyzer.get_30min_id(fdt)\n",
    "\n",
    "\n",
    "last_5min_id = nyc_taxi_analyzer.get_5min_id(ldt)\n",
    "last_15min_id = nyc_taxi_analyzer.get_15min_id(ldt)\n",
    "last_30min_id = nyc_taxi_analyzer.get_30min_id(ldt)\n",
    "\n",
    "\n",
    "train_valid_split_5min_id = nyc_taxi_analyzer.get_5min_id(train_valid_split_datetime)\n",
    "train_valid_split_15min_id = nyc_taxi_analyzer.get_15min_id(train_valid_split_datetime)\n",
    "train_valid_split_30min_id = nyc_taxi_analyzer.get_30min_id(train_valid_split_datetime)\n",
    "\n",
    "nyc_taxi_analyzer.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Question 2]\n",
    "TODO: load Manhattan data: from 2018-04 to 2018-06, call filter_abnormal_data to filter data\n",
    "\n",
    "filter_abnormal_data is defined in contest_helper.NycTaxiAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please put your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate [Question 2]\n",
    "print('sample_manhattan:', sample_manhattan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter too large total_amount, trip_distance, trip_duration\n",
    "start = time.time()\n",
    "\n",
    "m = np.mean(sample_manhattan['total_amount'])\n",
    "s = np.std(sample_manhattan['total_amount'])\n",
    "print('total_amount m:', m, 's:', s)\n",
    "sample_manhattan = sample_manhattan[sample_manhattan['total_amount'] <= m + 10*s]\n",
    "#sample_manhattan = sample_manhattan[sample_manhattan['total_amount'] >= m - 10*s]\n",
    "print('filter total_amount:', sample_manhattan.shape, time.time()-start)\n",
    "\n",
    "m = np.mean(sample_manhattan['trip_distance'])\n",
    "s = np.std(sample_manhattan['trip_distance'])\n",
    "print('trip_distance m:', m, 's:', s)\n",
    "sample_manhattan = sample_manhattan[sample_manhattan['trip_distance'] <= m + 20*s]\n",
    "#sample_manhattan = sample_manhattan[sample_manhattan['trip_distance'] >= m - 20*s]\n",
    "print('filter trip_distance:', sample_manhattan.shape, time.time()-start)\n",
    "\n",
    "m = np.mean(sample_manhattan['trip_duration'])\n",
    "s = np.std(sample_manhattan['trip_duration'])\n",
    "print('trip_duration m:', m, 's:', s)\n",
    "sample_manhattan = sample_manhattan[sample_manhattan['trip_duration'] <= m + 2*s]\n",
    "#sample_manhattan = sample_manhattan[sample_manhattan['trip_duration'] >= m - 2*s]\n",
    "print('filter trip_duration:', sample_manhattan.shape, time.time()-start)\n",
    "\n",
    "print('sample_manhattan.shape:', sample_manhattan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append, change and drop columns\n",
    "start = time.time()\n",
    "sample_manhattan['store_and_fwd_flag'] = sample_manhattan['store_and_fwd_flag'].map(lambda x: x == 'N' and 0 or 1)\n",
    "print('store_and_fwd_flag:', time.time()-start)\n",
    "\n",
    "sample_manhattan['tpep_pickup_5min_id'] = (sample_manhattan['tpep_pickup_datetime']-first_datetime).dt.total_seconds()//(5*60)\n",
    "print('tpep_pickup_5min_id:', time.time()-start)\n",
    "sample_manhattan['tpep_pickup_15min_id'] = (sample_manhattan['tpep_pickup_datetime']-first_datetime).dt.total_seconds()//(15*60)\n",
    "print('tpep_pickup_15min_id:', time.time()-start)\n",
    "sample_manhattan['tpep_pickup_30min_id'] = (sample_manhattan['tpep_pickup_datetime']-first_datetime).dt.total_seconds()//(30*60)\n",
    "print('tpep_pickup_30min_id:', time.time()-start)\n",
    "\n",
    "sample_manhattan.drop(['tpep_pickup_datetime', 'tpep_dropoff_datetime'], axis=1, inplace=True)  # , 'tpep_pickup_date', 'tpep_dropoff_date'\n",
    "print('sample_manhattan.shape:', sample_manhattan.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Question 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Show first 5 rows of sample_manhattan [Question 3.1]\n",
    "\n",
    "# please put your solution here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Show statistics of sample_manhattan [Question 3.2]\n",
    "\n",
    "# please put your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Challenge Question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add new prediction algorithm or change parameters of above 4 prediction algorithms\n",
    "\n",
    "# please put your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_group(id_name):\n",
    "    sample_group = sample_manhattan.groupby([id_name, 'PULocationID'])\n",
    "    sample_count = sample_group.count()\n",
    "    sample_count.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_count:', sample_count.shape)\n",
    "    sample_mean = sample_group.mean()\n",
    "    sample_mean.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_mean:', sample_mean.shape)\n",
    "    sample_sum = sample_group.sum()\n",
    "    sample_sum.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_sum:', sample_sum.shape)\n",
    "    sample_dropoff_group = sample_manhattan.groupby([id_name, 'DOLocationID'])\n",
    "    sample_dropoff_count = sample_dropoff_group.count()\n",
    "    sample_dropoff_count.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_dropoff_count:', sample_dropoff_count.shape)\n",
    "    sample_dropoff_mean = sample_dropoff_group.mean()\n",
    "    sample_dropoff_mean.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_dropoff_mean:', sample_dropoff_mean.shape)\n",
    "    sample_dropoff_sum = sample_dropoff_group.sum()\n",
    "    sample_dropoff_sum.index.rename([id_name, 'LocationID'], inplace=True)\n",
    "    print('sample_dropoff_sum:', sample_dropoff_sum.shape)\n",
    "    return sample_count, sample_mean, sample_sum, sample_dropoff_count, sample_dropoff_mean, sample_dropoff_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_count, sample_5min_mean, sample_5min_sum, sample_5min_dropoff_count, sample_5min_dropoff_mean, sample_5min_dropoff_sum = get_sample_group('tpep_pickup_5min_id')\n",
    "sample_15min_count, sample_15min_mean, sample_15min_sum, sample_15min_dropoff_count, sample_15min_dropoff_mean, sample_15min_dropoff_sum = get_sample_group('tpep_pickup_15min_id')\n",
    "sample_30min_count, sample_30min_mean, sample_30min_sum, sample_30min_dropoff_count, sample_30min_dropoff_mean, sample_30min_dropoff_sum = get_sample_group('tpep_pickup_30min_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_dropoff_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_5min_dropoff_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_5min_dropoff_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(all_index, sample_count, sample_mean, sample_sum, sample_dropoff_count, sample_dropoff_mean, sample_dropoff_sum):\n",
    "    all_count = all_index.join(sample_count, how='left')\n",
    "    all_count.fillna(0, inplace=True)\n",
    "    print('all_count:', all_count.shape)\n",
    "    all_mean = all_index.join(sample_mean, how='left')\n",
    "    all_mean.fillna(0, inplace=True)\n",
    "    print('all_mean:', all_mean.shape)\n",
    "    all_sum = all_index.join(sample_sum, how='left')\n",
    "    all_sum.fillna(0, inplace=True)\n",
    "    print('all_sum:', all_sum.shape)\n",
    "    all_dropoff_count = all_index.join(sample_dropoff_count, how='left')\n",
    "    all_dropoff_count.fillna(0, inplace=True)\n",
    "    print('all_dropoff_count:', all_dropoff_count.shape)\n",
    "    all_dropoff_mean = all_index.join(sample_dropoff_mean, how='left')\n",
    "    all_dropoff_mean.fillna(0, inplace=True)\n",
    "    print('all_dropoff_mean:', all_dropoff_mean.shape)\n",
    "    all_dropoff_sum = all_index.join(sample_dropoff_sum, how='left')\n",
    "    all_dropoff_sum.fillna(0, inplace=True)\n",
    "    print('all_dropoff_sum:', all_dropoff_sum.shape)\n",
    "    all_xmin = all_count.copy()\n",
    "    all_xmin = all_xmin.join(all_mean, lsuffix='_count', rsuffix='_mean')\n",
    "    all_xmin = all_xmin.join(all_sum, rsuffix='_sum')\n",
    "    all_xmin = all_xmin.join(all_dropoff_count, rsuffix='_dropoff_count')\n",
    "    all_xmin = all_xmin.join(all_dropoff_mean, rsuffix='_dropoff_mean')\n",
    "    all_xmin = all_xmin.join(all_dropoff_sum, rsuffix='_dropoff_sum')\n",
    "    print('all_xmin:', all_xmin.shape)\n",
    "    return all_xmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min = get_all(all_5min_index, sample_5min_count, sample_5min_mean, sample_5min_sum, sample_5min_dropoff_count, sample_5min_dropoff_mean, sample_5min_dropoff_sum)\n",
    "all_15min = get_all(all_15min_index, sample_15min_count, sample_15min_mean, sample_15min_sum, sample_15min_dropoff_count, sample_15min_dropoff_mean, sample_15min_dropoff_sum)\n",
    "all_30min = get_all(all_30min_index, sample_30min_count, sample_30min_mean, sample_30min_sum, sample_30min_dropoff_count, sample_30min_dropoff_mean, sample_30min_dropoff_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(all_xmin, all_static):\n",
    "    all_xmin_features = all_static.copy()\n",
    "    all_xmin_features['value'] = all_xmin['VendorID_count']\n",
    "    all_xmin_features['5min_ago'] = all_xmin['VendorID_count'].shift(manhattan_location_num)\n",
    "    all_xmin_features['5min_10min_ago'] = all_xmin['VendorID_count'].shift(2*manhattan_location_num)\n",
    "    all_xmin_features['10min_ago'] = all_xmin_features['5min_ago'] + all_xmin_features['5min_10min_ago']\n",
    "    all_xmin_features['10min_15min_ago'] = all_xmin['VendorID_count'].shift(3*manhattan_location_num)\n",
    "    all_xmin_features['15min_ago'] = all_xmin_features['10min_ago'] + all_xmin_features['10min_15min_ago']\n",
    "    all_xmin_features['15min_20min_ago'] = all_xmin['VendorID_count'].shift(4*manhattan_location_num)\n",
    "    all_xmin_features['20min_ago'] = all_xmin_features['15min_ago'] + all_xmin_features['15min_20min_ago']\n",
    "    all_xmin_features['20min_25min_ago'] = all_xmin['VendorID_count'].shift(5*manhattan_location_num)\n",
    "    all_xmin_features['25min_ago'] = all_xmin_features['20min_ago'] + all_xmin_features['20min_25min_ago']\n",
    "    all_xmin_features['25min_30min_ago'] = all_xmin['VendorID_count'].shift(6*manhattan_location_num)\n",
    "    all_xmin_features['30min_ago'] = all_xmin_features['25min_ago'] + all_xmin_features['25min_30min_ago']\n",
    "    all_xmin_features['5min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(manhattan_location_num)\n",
    "    all_xmin_features['5min_10min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(2*manhattan_location_num)\n",
    "    all_xmin_features['10min_ago_drop'] = all_xmin_features['5min_ago_drop'] + all_xmin_features['5min_10min_ago_drop']\n",
    "    all_xmin_features['10min_15min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(3*manhattan_location_num)\n",
    "    all_xmin_features['15min_ago_drop'] = all_xmin_features['10min_ago_drop'] + all_xmin_features['10min_15min_ago_drop']\n",
    "    all_xmin_features['15min_20min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(4*manhattan_location_num)\n",
    "    all_xmin_features['20min_ago_drop'] = all_xmin_features['15min_ago_drop'] + all_xmin_features['15min_20min_ago_drop']\n",
    "    all_xmin_features['20min_25min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(5*manhattan_location_num)\n",
    "    all_xmin_features['25min_ago_drop'] = all_xmin_features['20min_ago_drop'] + all_xmin_features['20min_25min_ago_drop']\n",
    "    all_xmin_features['25min_30min_ago_drop'] = all_xmin['VendorID_dropoff_count'].shift(6*manhattan_location_num)\n",
    "    all_xmin_features['30min_ago_drop'] = all_xmin_features['25min_ago_drop'] + all_xmin_features['25min_30min_ago_drop']\n",
    "    all_xmin_features['1day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24)\n",
    "    all_xmin_features['7day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24*7)\n",
    "    all_xmin_features['14day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24*14)\n",
    "    all_xmin_features['21day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24*21)\n",
    "    all_xmin_features['28day_ago_now'] = all_xmin['VendorID_count'].shift(manhattan_location_num*12*24*28)\n",
    "    all_xmin_features.fillna(0, inplace=True)\n",
    "    print('all_xmin_features:', all_xmin_features.shape)\n",
    "    return all_xmin_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_features = get_all_features(all_5min, all_5min_static)\n",
    "all_15min_features = get_all_features(all_15min, all_15min_static)\n",
    "all_30min_features = get_all_features(all_30min, all_30min_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_5min_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validate\n",
    "\n",
    "We split all data into train and validate part. We demonstrate 4 methods to forecast requests: XGBoost, LightGBM, linear regression implemented using sklearn and linear regression implemented using TensorFlow, and evaluate the models using mean absolute error (MAE). We also visualize the prediction results between 2018-01-01 00:00:00 and 2018-01-01 00:05:00 using `geopandas` (the darker the color, the more demand), and we can visualize any time slot using this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_5min = all_5min_features[:int(train_valid_split_5min_id)*manhattan_location_num]\n",
    "print('train_X_5min:', train_X_5min.shape)\n",
    "valid_X_5min = all_5min_features[int(train_valid_split_5min_id)*manhattan_location_num:int(last_5min_id)*manhattan_location_num]\n",
    "print('valid_X_5min:', valid_X_5min.shape)\n",
    "train_Y_5min = train_X_5min['value'].values\n",
    "print('train_Y_5min:', len(train_Y_5min))\n",
    "valid_Y_5min = valid_X_5min['value'].values\n",
    "print('valid_Y_5min:', len(valid_Y_5min))\n",
    "\n",
    "train_X_15min = all_15min_features[:int(train_valid_split_15min_id)*manhattan_location_num]\n",
    "print('train_X_15min:', train_X_15min.shape)\n",
    "valid_X_15min = all_15min_features[int(train_valid_split_15min_id)*manhattan_location_num:int(last_15min_id)*manhattan_location_num]\n",
    "print('valid_X_15min:', valid_X_15min.shape)\n",
    "train_Y_15min = train_X_15min['value'].values\n",
    "print('train_Y_15min:', len(train_Y_15min))\n",
    "valid_Y_15min = valid_X_15min['value'].values\n",
    "print('valid_Y_15min:', len(valid_Y_15min))\n",
    "\n",
    "train_X_30min = all_30min_features[:int(train_valid_split_30min_id)*manhattan_location_num]\n",
    "print('train_X_30min:', train_X_30min.shape)\n",
    "valid_X_30min = all_30min_features[int(train_valid_split_30min_id)*manhattan_location_num:int(last_30min_id)*manhattan_location_num]\n",
    "print('valid_X_30min:', valid_X_30min.shape)\n",
    "train_Y_30min = train_X_30min['value'].values\n",
    "print('train_Y_30min:', len(train_Y_30min))\n",
    "valid_Y_30min = valid_X_30min['value'].values\n",
    "print('valid_Y_30min:', len(valid_Y_30min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump((train_X_5min, valid_X_5min, train_Y_5min, valid_Y_5min), open('train_valid_5min.pickle', 'wb'), protocol=2)\n",
    "pickle.dump((train_X_15min, valid_X_15min, train_Y_15min, valid_Y_15min), open('train_valid_15min.pickle', 'wb'), protocol=2)\n",
    "pickle.dump((train_X_30min, valid_X_30min, train_Y_30min, valid_Y_30min), open('train_valid_30min.pickle', 'wb'), protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_5min, valid_X_5min, train_Y_5min, valid_Y_5min = pickle.load(open('train_valid_5min.pickle', 'rb'))\n",
    "train_X_15min, valid_X_15min, train_Y_15min, valid_Y_15min = pickle.load(open('train_valid_15min.pickle', 'rb'))\n",
    "train_X_30min, valid_X_30min, train_Y_30min, valid_Y_30min = pickle.load(open('train_valid_30min.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_5min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_5min.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X_5min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X_5min.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    xg_train = xgb.DMatrix(train_X.drop('value', axis=1), label=train_Y)\n",
    "    xg_test = xgb.DMatrix(test_X.drop('value', axis=1), label=test_Y)\n",
    "    # setup parameters for xgboost\n",
    "    param = {}\n",
    "    # scale weight of positive examples\n",
    "    param['eta'] = 0.1  # default\n",
    "    param['max_depth'] = 6  # default: 6\n",
    "    param['silent'] = 1  # default\n",
    "    param['nthread'] = 4  # default\n",
    "    param['gamma'] = 1\n",
    "    param['subsample'] = 0.9\n",
    "    param['min_child_weight'] = 1\n",
    "    param['colsample_bytree'] = 0.9\n",
    "    param['lambda'] = 1\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['eval_metric'] = 'mae'\n",
    "    param['objective'] = 'reg:linear'\n",
    "    \n",
    "    watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "    num_round = 100\n",
    "\n",
    "    bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "\n",
    "    imp = bst.get_fscore()\n",
    "    print(sorted(imp.items(), key=lambda d: d[1], reverse=True))\n",
    "    \n",
    "    pred = bst.predict(xg_test)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    rfc = LinearRegression()\n",
    "    rfc.fit(train_X.drop('value', axis=1), train_Y.astype(np.float))\n",
    "    pred = rfc.predict(test_X.drop('value', axis=1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=tf.nn.relu, input_shape=[train_X_5min.shape[1]-1]),\n",
    "        layers.Dense(64, activation=tf.nn.relu),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.001)\n",
    "\n",
    "    model.compile(loss='mae', optimizer=optimizer, metrics=['mae', 'mse'])\n",
    "    return model\n",
    "\n",
    "def tf_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    EPOCHS = 10\n",
    "    model = build_model()\n",
    "    model.summary()\n",
    "    history = model.fit(train_X.drop('value', axis=1), train_Y, epochs=EPOCHS, validation_split = 0.2, verbose=2)\n",
    "    loss, mae, mse = model.evaluate(test_X.drop('value', axis=1), test_Y, verbose=1)\n",
    "    pred = model.predict(test_X.drop('value', axis=1)).flatten()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_train_validate(train_X, train_Y, test_X, test_Y):\n",
    "    # create dataset for lightgbm\n",
    "    lgb_train = lgb.Dataset(train_X.drop('value', axis=1), train_Y)\n",
    "    lgb_eval = lgb.Dataset(test_X.drop('value', axis=1), test_Y, reference=lgb_train)\n",
    "\n",
    "    # specify your configurations as a dict\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': {'l2', 'l1'},\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    print('Starting training...')\n",
    "    # train\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=100,\n",
    "                    valid_sets=lgb_eval,\n",
    "                    early_stopping_rounds=5)\n",
    "    \n",
    "    print('Starting predicting...')\n",
    "    # predict\n",
    "    pred = gbm.predict(test_X.drop('value', axis=1), num_iteration=gbm.best_iteration)\n",
    "    # eval\n",
    "    print('The mae of prediction is:', mae(test_Y, pred))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_5min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred_5min_group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
